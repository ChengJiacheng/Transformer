{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: Transformers and Multi-Head Attention\n",
    "\n",
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
    "\n",
    "\n",
    "**Filled notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.ipynb)  \n",
    "**Pre-trained models:** \n",
    "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/tutorial6)\n",
    "[![GoogleDrive](https://img.shields.io/static/v1.svg?logo=google-drive&logoColor=yellow&label=GDrive&message=Download&color=yellow)](https://drive.google.com/drive/folders/1DF7POc6j03pRiWQPWSl5QJX5iY-xK0sV?usp=sharing)   \n",
    "**Recordings:** \n",
    "[![YouTube - Part 1](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%201&color=red)](https://youtu.be/hGZ6wa07Vak)\n",
    "[![YouTube - Part 2](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%202&color=red)](https://youtu.be/QdTgJ85E6YA)\n",
    "[![YouTube - Part 3](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%203&color=red)](https://youtu.be/e7xvF2yS4Dg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will discuss one of the most impactful architectures of the last 2 years: the Transformer model. Since the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. had been published in 2017, the Transformer architecture has continued to beat benchmarks in many domains, most importantly in Natural Language Processing. Transformers with an incredible amount of parameters can generate long, convincing [essays](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3), and opened up new application fields of AI. As the hype of the Transformer architecture seems not to come to an end in the next years, it is important to understand how it works, and have implemented it yourself, which we will do in this notebook.\n",
    "\n",
    "Despite the huge success of Transformers in NLP, we will _not_ include the NLP domain in our notebook here. Why? Firstly, the Master AI at UvA offers many great NLP courses that will take a closer look at the application of the Transformer architecture in NLP ([NLP2](https://studiegids.uva.nl/xmlpages/page/2020-2021/zoek-vak/vak/79628), [Advanced Topics in Computational Semantics](https://studiegids.uva.nl/xmlpages/page/2020-2021/zoek-vak/vak/80162)). Secondly, assignment 2 takes already a closer look at language generation on character level, on which you could easily apply our transformer architecture. Finally, and most importantly, there is so much more to the Transformer architecture. NLP is the domain the Transformer architecture has been originally proposed for and had the greatest impact on, but it also accelerated research in other domains, recently even [Computer Vision](https://arxiv.org/abs/2010.11929). Thus, we focus here on what makes the Transformer and self-attention so powerful in general. In [Tutorial 15](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html), we will discuss the application of Transformers in Computer Vision.\n",
    "\n",
    "Below, we import our standard libraries. Similarly as in Tutorial 5, we will use [PyTorch Lightning](https://www.pytorchlightning.ai/) as an additional framework. If you are not familiar with PyTorch Lightning, please make sure to have read Tutorial 5 carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiacheng\\AppData\\Local\\Temp\\ipykernel_15448\\1034402887.py:14: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np \n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial6\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two pre-trained models are downloaded below. Make sure to have adjusted your `CHECKPOINT_PATH` before running this code if not already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial6/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"ReverseTask.ckpt\", \"SetAnomalyTask.ckpt\"]\n",
    "\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer architecture\n",
    "\n",
    "In the first part of this notebook, we will implement the Transformer architecture by hand. As the architecture is so popular, there already exists a Pytorch module `nn.Transformer` ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)) and a [tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) on how to use it for next token prediction. However, we will implement it here ourselves, to get through to the smallest details.\n",
    "\n",
    "There are of course many more tutorials out there about attention and Transformers. Below, we list a few that are worth exploring if you are interested in the topic and might want yet another perspective on the topic after this one:\n",
    "\n",
    "* [Transformer: A Novel Neural Network Architecture for Language Understanding (Jakob Uszkoreit, 2017)](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) - The original Google blog post about the Transformer paper, focusing on the application in machine translation.\n",
    "* [The Illustrated Transformer (Jay Alammar, 2018)](http://jalammar.github.io/illustrated-transformer/) - A very popular and great blog post intuitively explaining the Transformer architecture with many nice visualizations. The focus is on NLP.\n",
    "* [Attention? Attention! (Lilian Weng, 2018)](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) - A nice blog post summarizing attention mechanisms in many domains including vision.\n",
    "* [Illustrated: Self-Attention (Raimi Karim, 2019)](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) - A nice visualization of the steps of self-attention. Recommended going through if the explanation below is too abstract for you.\n",
    "* [The Transformer family (Lilian Weng, 2020)](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html) - A very detailed blog post reviewing more variants of Transformers besides the original one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Attention?\n",
    "\n",
    "The attention mechanism describes a recent new group of layers in neural networks that has attracted a lot of interest in the past few years, especially in sequence tasks. There are a lot of different possible definitions of \"attention\" in the literature, but the one we will use here is the following: _the attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements' keys_. So what does this exactly mean? The goal is to take an average over the features of multiple elements. However, instead of weighting each element equally, we want to weight them depending on their actual values. In other words, we want to dynamically decide on which inputs we want to \"attend\" more than others. In particular, an attention mechanism has usually four parts we need to specify:\n",
    "\n",
    "* **Query**: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n",
    "* **Keys**: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is \"offering\", or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n",
    "* **Values**: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
    "* **Score function**: To rate which elements we want to pay attention to, we need to specify a score function $f_{attn}$. The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n",
    "\n",
    "\n",
    "The weights of the average are calculated by a softmax over all score function outputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query. If we try to describe it with pseudo-math, we can write: \n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{key}_i, \\text{query}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{key}_j, \\text{query}\\right)\\right)}, \\hspace{5mm} \\text{out} = \\sum_i \\alpha_i \\cdot \\text{value}_i\n",
    "$$\n",
    "\n",
    "Visually, we can show the attention over a sequence of words as follows:\n",
    "\n",
    "<center width=\"100%\" style=\"padding:25px\"><img src=\"attention_example.svg\" width=\"750px\"></center>\n",
    "\n",
    "For every word, we have one key and one value vector. The query is compared to all keys with a score function (in this case the dot product) to determine the weights. The softmax is not visualized for simplicity. Finally, the value vectors of all words are averaged using the attention weights.\n",
    "\n",
    "Most attention mechanisms differ in terms of what queries they use, how the key and value vectors are defined, and what score function is used. The attention applied inside the Transformer architecture is called **self-attention**. In self-attention, each sequence element provides a key, value, and query. For each element, we perform an attention layer where based on its query, we check the similarity of the all sequence elements' keys, and returned a different, averaged value vector for each element. We will now go into a bit more detail by first looking at the specific implementation of the attention mechanism which is in the Transformer case the scaled dot product attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention\n",
    "\n",
    "The core concept behind self-attention is the scaled dot product attention. Our goal is to have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute. The dot product attention takes as input a set of queries $Q\\in\\mathbb{R}^{T\\times d_k}$, keys $K\\in\\mathbb{R}^{T\\times d_k}$ and values $V\\in\\mathbb{R}^{T\\times d_v}$ where $T$ is the sequence length, and $d_k$ and $d_v$ are the hidden dimensionality for queries/keys and values respectively. For simplicity, we neglect the batch dimension for now. The attention value from element $i$ to $j$ is based on its similarity of the query $Q_i$ and key $K_j$, using the dot product as the similarity metric. In math, we calculate the dot product attention as follows:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The matrix multiplication $QK^T$ performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape $T\\times T$. Each row represents the attention logits for a specific element $i$ to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention). Another perspective on this attention mechanism offers the computation graph which is visualized below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
    "\n",
    "<center width=\"100%\"><img src=\"scaled_dot_product_attn.svg\" width=\"210px\"></center>\n",
    "\n",
    "One aspect we haven't discussed yet is the scaling factor of $1/\\sqrt{d_k}$. This scaling factor is crucial to maintain an appropriate variance of attention values after initialization. Remember that we intialize our layers with the intention of having equal variance throughout the model, and hence, $Q$ and $K$ might also have a variance close to $1$. However, performing a dot product over two vectors with a variance $\\sigma^2$ results in a scalar having $d_k$-times higher variance: \n",
    "\n",
    "$$q_i \\sim \\mathcal{N}(0,\\sigma^2), k_i \\sim \\mathcal{N}(0,\\sigma^2) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma^2\\cdot d_k$$\n",
    "\n",
    "\n",
    "If we do not scale down the variance back to $\\sigma^2$, the softmax over the logits will already saturate to $1$ for one random element and $0$ for all others. The gradients through the softmax will be close to zero so that we can't learn the parameters appropriately. \n",
    "\n",
    "The block `Mask (opt.)` in the diagram above represents the optional masking of specific entries in the attention matrix. This is for instance used if we stack multiple sequences with different lengths into a batch. To still benefit from parallelization in PyTorch, we pad the sentences to the same length and mask out the padding tokens during the calculation of the attention values. This is usually done by setting the respective attention logits to a very low value. \n",
    "\n",
    "After we have discussed the details of the scaled dot product attention block, we can write a function below which computes the output features given the triple of queries, keys, and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our code above supports any additional dimensionality in front of the sequence length so that we can also use it for batches. However, for a better understanding, let's generate a few random queries, keys, and value vectors, and calculate the attention outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " tensor([[ 0.3367,  0.1288],\n",
      "        [ 0.2345,  0.2303],\n",
      "        [-1.1229, -0.1863]])\n",
      "K\n",
      " tensor([[ 2.2082, -0.6380],\n",
      "        [ 0.4617,  0.2674],\n",
      "        [ 0.5349,  0.8094]])\n",
      "V\n",
      " tensor([[ 1.1103, -1.6898],\n",
      "        [-0.9890,  0.9580],\n",
      "        [ 1.3221,  0.8172]])\n",
      "Values\n",
      " tensor([[ 0.5698, -0.1520],\n",
      "        [ 0.5379, -0.0265],\n",
      "        [ 0.2246,  0.5556]])\n",
      "Attention\n",
      " tensor([[0.4028, 0.2886, 0.3086],\n",
      "        [0.3538, 0.3069, 0.3393],\n",
      "        [0.1303, 0.4630, 0.4067]])\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "pl.seed_everything(42)\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, make sure you can follow the calculation of the specific values here, and also check it by hand. It is important to fully understand how the scaled dot product attention is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features. Specifically, given a query, key, and value matrix, we transform those into $h$ sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix. Mathematically, we can express this operation as:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
    "    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "We refer to this as Multi-Head Attention layer with the learnable parameters $W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}$, and $W^{O}\\in\\mathbb{R}^{h\\cdot d_k\\times d_{out}}$ ($D$ being the input dimensionality). Expressed in a computational graph, we can visualize it as below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
    "\n",
    "<center width=\"100%\"><img src=\"multihead_attention.svg\" width=\"230px\"></center>\n",
    "\n",
    "How are we applying a Multi-Head Attention layer in a neural network, where we don't have an arbitrary query, key, and value vector as input? Looking at the computation graph above, a simple but effective implementation is to set the current feature map in a NN, $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$, as $Q$, $K$ and $V$ ($B$ being the batch size, $T$ the sequence length, $d_{\\text{model}}$ the hidden dimensionality of $X$). The consecutive weight matrices $W^{Q}$, $W^{K}$, and $W^{V}$ can transform $X$ to the corresponding feature vectors that represent the queries, keys, and values of the input. Using this approach, we can implement the Multi-Head Attention module below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "        \n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "        \n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One crucial characteristic of the multi-head attention is that it is permutation-equivariant with respect to its inputs. This means that if we switch two input elements in the sequence, e.g. $X_1\\leftrightarrow X_2$ (neglecting the batch dimension for now), the output is exactly the same besides the elements 1 and 2 switched. Hence, the multi-head attention is actually looking at the input not as a sequence, but as a set of elements. This property makes the multi-head attention block and the Transformer architecture so powerful and widely applicable! But what if the order of the input is actually important for solving the task, like language modeling? The answer is to encode the position in the input features, which we will take a closer look at later (topic _Positional encodings_ below).\n",
    "\n",
    "Before moving on to creating the Transformer architecture, we can compare the self-attention operation with our other common layer competitors for sequence data: convolutions and recurrent neural networks. Below you can find a table by [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) on the complexity per layer, the number of sequential operations, and maximum path length. The complexity is measured by the upper bound of the number of operations to perform, while the maximum path length represents the maximum number of steps a forward or backward signal has to traverse to reach any other position. The lower this length, the better gradient signals can backpropagate for long-range dependencies. Let's take a look at the table below:\n",
    "\n",
    "\n",
    "<center width=\"100%\"><img src=\"comparison_conv_rnn.svg\" width=\"600px\"></center>\n",
    "\n",
    "$n$ is the sequence length, $d$ is the representation dimension and $k$ is the kernel size of convolutions. In contrast to recurrent networks, the self-attention layer can parallelize all its operations making it much faster to execute for smaller sequence lengths. However, when the sequence length exceeds the hidden dimensionality, self-attention becomes more expensive than RNNs. One way of reducing the computational cost for long sequences is by restricting the self-attention to a neighborhood of inputs to attend over, denoted by $r$. Nevertheless, there has been recently a lot of work on more efficient Transformer architectures that still allow long dependencies, of which you can find an overview in the paper by [Tay et al. (2020)](https://arxiv.org/abs/2009.06732) if interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder\n",
    "\n",
    "Next, we will look at how to apply the multi-head attention block inside the Transformer architecture. Originally, the Transformer model was designed for machine translation. Hence, it got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation. On the other hand, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner, as in a standard RNN. While this structure is extremely useful for Sequence-to-Sequence tasks with the necessity of autoregressive decoding, we will focus here on the encoder part. Many advances in NLP have been made using pure encoder-based Transformer models (if interested, models include the [BERT](https://arxiv.org/abs/1810.04805)-family, the [Vision Transformer](https://arxiv.org/abs/2010.11929), and more), and in our tutorial, we will also mainly focus on the encoder part. If you have understood the encoder architecture, the decoder is a very small step to implement as well. The full Transformer architecture looks as follows (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).:\n",
    "\n",
    "<center width=\"100%\"><img src=\"transformer_architecture.svg\" width=\"400px\"></center>\n",
    "\n",
    "The encoder consists of $N$ identical blocks that are applied in sequence. Taking as input $x$, it is first passed through a Multi-Head Attention block as we have implemented above. The output is added to the original input using a residual connection, and we apply a consecutive Layer Normalization on the sum. Overall, it calculates $\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))$ ($x$ being $Q$, $K$ and $V$ input to the attention layer). The residual connection is crucial in the Transformer architecture for two reasons: \n",
    "\n",
    "1. Similar to ResNets, Transformers are designed to be very deep. Some models contain more than 24 blocks in the encoder. Hence, the residual connections are crucial for enabling a smooth gradient flow through the model.\n",
    "2. Without the residual connection, the information about the original sequence is lost. Remember that the Multi-Head Attention layer ignores the position of elements in a sequence, and can only learn it based on the input features. Removing the residual connections would mean that this information is lost after the first attention layer (after initialization), and with a randomly initialized query and key vector, the output vectors for position $i$ has no relation to its original input. All outputs of the attention are likely to represent similar/same information, and there is no chance for the model to distinguish which information came from which input element. An alternative option to residual connection would be to fix at least one head to focus on its original input, but this is very inefficient and does not have the benefit of the improved gradient flow.\n",
    "\n",
    "The Layer Normalization also plays an important role in the Transformer architecture as it enables faster training and provides small regularization. Additionally, it ensures that the features are in a similar magnitude among the elements in the sequence. We are not using Batch Normalization because it depends on the batch size which is often small with Transformers (they require a lot of GPU memory), and BatchNorm has shown to perform particularly bad in language as the features of words tend to have a much higher variance (there are many, very rare words which need to be considered for a good distribution estimate).\n",
    "\n",
    "Additionally to the Multi-Head Attention, a small fully connected feed-forward network is added to the model, which is applied to each position separately and identically. Specifically, the model uses a Linear$\\to$ReLU$\\to$Linear MLP. The full transformation including the residual connection can be expressed as:  \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n",
    "    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "This MLP adds extra complexity to the model and allows transformations on each sequence element separately. You can imagine as this allows the model to \"post-process\" the new information added by the previous Multi-Head Attention, and prepare it for the next attention block. Usually, the inner dimensionality of the MLP is 2-8$\\times$ larger than $d_{\\text{model}}$, i.e. the dimensionality of the original input $x$. The general advantage of a wider layer instead of a narrow, multi-layer MLP is the faster, parallelizable execution.\n",
    "\n",
    "Finally, after looking at all parts of the encoder architecture, we can start implementing it below. We first start by implementing a single encoder block. Additionally to the layers described above, we will add dropout layers in the MLP and on the output of the MLP and Multi-Head Attention for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "        \n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "        \n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this block, we can implement a module for the full Transformer encoder. Additionally to a forward function that iterates through the sequence of encoder blocks, we also provide a function called `get_attention_maps`. The idea of this function is to return the attention probabilities for all Multi-Head Attention blocks in the encoder. This helps us in understanding, and in a sense, explaining the model. However, the attention probabilities should be interpreted with a grain of salt as it does not necessarily reflect the true interpretation of the model (there is a series of papers about this, including [Attention is not Explanation](https://arxiv.org/abs/1902.10186) and [Attention is not not Explanation](https://arxiv.org/abs/1908.04626))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for l in self.layers:\n",
    "            x = l(x, mask=mask)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "\n",
    "We have discussed before that the Multi-Head Attention block is permutation-equivariant, and cannot distinguish whether an input comes before another one in the sequence or not. In tasks like language understanding, however, the position is important for interpreting the input words. The position information can therefore be added via the input features. We could learn a embedding for every possible position, but this would not generalize to a dynamical input sequence length. Hence, the better option is to use feature patterns that the network can identify from the features and potentially generalize to larger sequences. The specific pattern chosen by Vaswani et al. are sine and cosine functions of different frequencies, as follows:\n",
    "\n",
    "$$\n",
    "PE_{(pos,i)} = \\begin{cases}\n",
    "    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n",
    "    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$PE_{(pos,i)}$ represents the position encoding at position $pos$ in the sequence, and hidden dimensionality $i$. These values, concatenated for all hidden dimensions, are added to the original input features (in the Transformer visualization above, see \"Positional encoding\"), and constitute the position information. We distinguish between even ($i \\text{ mod } 2=0$) and uneven ($i \\text{ mod } 2=1$) hidden dimensionalities where we apply a sine/cosine respectively. The intuition behind this encoding is that you can represent $PE_{(pos+k,:)}$ as a linear function of $PE_{(pos,:)}$, which might allow the model to easily attend to relative positions. The wavelengths in different dimensions range from $2\\pi$ to $10000\\cdot 2\\pi$.\n",
    "\n",
    "The positional encoding is implemented below. The code is taken from the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model) about Transformers on NLP and adjusted for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model) \n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the positional encoding, we can visualize it below. We will generate an image of the positional encoding over hidden dimensionality and position in a sequence. Each pixel, therefore, represents the change of the input feature we perform to encode the specific position. Let's do it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"442.082438pt\" height=\"222.954375pt\" viewBox=\"0 0 442.082438 222.954375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-04-08T00:12:02.113406</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 222.954375 \n",
       "L 442.082438 222.954375 \n",
       "L 442.082438 0 \n",
       "L 0 0 \n",
       "L 0 222.954375 \n",
       "z\n",
       "\" style=\"fill: none\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 40.603125 185.398125 \n",
       "L 366.763125 185.398125 \n",
       "L 366.763125 22.318125 \n",
       "L 40.603125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p0d94cdd80e)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"image75e50eec39\" transform=\"scale(1 -1)translate(0 -164)\" x=\"40.603125\" y=\"-21.398125\" width=\"327\" height=\"164\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mb5ba59a4cd\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb5ba59a4cd\" x=\"40.603125\" y=\"185.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(37.421875 199.996562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb5ba59a4cd\" x=\"71.180625\" y=\"185.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(64.818125 199.996562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb5ba59a4cd\" x=\"105.155625\" y=\"185.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(98.793125 199.996562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb5ba59a4cd\" x=\"139.130625\" y=\"185.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(132.768125 199.996562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb5ba59a4cd\" x=\"173.105625\" y=\"185.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(166.743125 199.996562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb5ba59a4cd\" x=\"207.080625\" y=\"185.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(200.718125 199.996562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb5ba59a4cd\" x=\"241.055625\" y=\"185.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 60 -->\n",
       "      <g transform=\"translate(234.693125 199.996562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb5ba59a4cd\" x=\"275.030625\" y=\"185.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 70 -->\n",
       "      <g transform=\"translate(268.668125 199.996562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb5ba59a4cd\" x=\"309.005625\" y=\"185.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 80 -->\n",
       "      <g transform=\"translate(302.643125 199.996562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb5ba59a4cd\" x=\"342.980625\" y=\"185.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 90 -->\n",
       "      <g transform=\"translate(336.618125 199.996562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- Position in sequence -->\n",
       "     <g transform=\"translate(152.387813 213.674688)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \n",
       "L 1259 2394 \n",
       "L 2053 2394 \n",
       "Q 2494 2394 2734 2622 \n",
       "Q 2975 2850 2975 3272 \n",
       "Q 2975 3691 2734 3919 \n",
       "Q 2494 4147 2053 4147 \n",
       "L 1259 4147 \n",
       "z\n",
       "M 628 4666 \n",
       "L 2053 4666 \n",
       "Q 2838 4666 3239 4311 \n",
       "Q 3641 3956 3641 3272 \n",
       "Q 3641 2581 3239 2228 \n",
       "Q 2838 1875 2053 1875 \n",
       "L 1259 1875 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <defs>\n",
       "       <path id=\"mc1cbd2b853\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc1cbd2b853\" x=\"40.603125\" y=\"22.318125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(27.240625 26.117344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc1cbd2b853\" x=\"40.603125\" y=\"52.895625\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(20.878125 56.694844)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc1cbd2b853\" x=\"40.603125\" y=\"86.870625\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(20.878125 90.669844)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc1cbd2b853\" x=\"40.603125\" y=\"120.845625\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(20.878125 124.644844)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc1cbd2b853\" x=\"40.603125\" y=\"154.820625\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(20.878125 158.619844)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- Hidden dimension -->\n",
       "     <g transform=\"translate(14.798438 149.090937)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-48\" d=\"M 628 4666 \n",
       "L 1259 4666 \n",
       "L 1259 2753 \n",
       "L 3553 2753 \n",
       "L 3553 4666 \n",
       "L 4184 4666 \n",
       "L 4184 0 \n",
       "L 3553 0 \n",
       "L 3553 2222 \n",
       "L 1259 2222 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"75.195312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"102.978516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"166.455078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"229.931641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"291.455078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"354.833984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"386.621094\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"450.097656\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"477.880859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"575.292969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"636.816406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"700.195312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"752.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"780.078125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"841.259766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.603125 185.398125 \n",
       "L 40.603125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 366.763125 185.398125 \n",
       "L 366.763125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 40.603125 185.398125 \n",
       "L 366.763125 185.398125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 40.603125 22.318125 \n",
       "L 366.763125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- Positional encoding over hidden dimensions -->\n",
       "    <g transform=\"translate(71.633438 16.318125)scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"969.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1001.111328\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"1062.292969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1121.472656\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"1182.996094\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1224.109375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"1255.896484\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1319.275391\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"1347.058594\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"1410.535156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1474.011719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1535.535156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1598.914062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"1630.701172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1694.177734\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1721.960938\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1819.373047\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1880.896484\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1944.275391\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1996.375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"2024.158203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"2085.339844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"2148.71875\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 389.083125 185.398125 \n",
       "L 397.237125 185.398125 \n",
       "L 397.237125 22.318125 \n",
       "L 389.083125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path clip-path=\"url(#pf762f80954)\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.01; stroke-linejoin: miter\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAgAAACjCAYAAAC31F+mAAAA/klEQVR4nO2WSw6DMAwF409yjd6v998BSRfdeypZWNCWbZ7GY8cg5CmP1YLHh0p03twEAh8QwvPmnUrkHfKSJYTT21Qc9dA4IAYBJmiWYMOoBLZJBLhOJmjaQTGAkj3bpnYnB7gsHUDAfdDR44CRJC4ttslrf76D0RwK2lQHh6a0cp0IPoBAC1NBMGiz0aAEAzRJLsGS6UEZ3SYGsIRmPyDsIEgAh4oSgh9zIvCo4y5kzRn/T4anrTWfc8aBtWKGH8eRJKBDQYl8ID+oK9xFASE/hwLJvMMtJNFh3/dkACV/hHALyTxh27ZkiTwBJfG9+Lf5DhRIXoFwC8mvaPMFhZj7x7gJVREAAAAASUVORK5CYII=\" id=\"image9ef4c4d1e6\" transform=\"scale(1 -1)translate(0 -163)\" x=\"389\" y=\"-21\" width=\"8\" height=\"163\"/>\n",
       "   <g id=\"matplotlib.axis_3\">\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <defs>\n",
       "       <path id=\"m4ee4aebc37\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4ee4aebc37\" x=\"397.237125\" y=\"165.013822\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- −0.75 -->\n",
       "      <g transform=\"translate(404.237125 168.813041)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4ee4aebc37\" x=\"397.237125\" y=\"144.628723\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- −0.50 -->\n",
       "      <g transform=\"translate(404.237125 148.427942)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4ee4aebc37\" x=\"397.237125\" y=\"124.243623\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_21\">\n",
       "      <!-- −0.25 -->\n",
       "      <g transform=\"translate(404.237125 128.042842)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4ee4aebc37\" x=\"397.237125\" y=\"103.858524\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- 0.00 -->\n",
       "      <g transform=\"translate(404.237125 107.657742)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4ee4aebc37\" x=\"397.237125\" y=\"83.473424\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(404.237125 87.272643)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4ee4aebc37\" x=\"397.237125\" y=\"63.088324\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(404.237125 66.887543)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4ee4aebc37\" x=\"397.237125\" y=\"42.703225\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- 0.75 -->\n",
       "      <g transform=\"translate(404.237125 46.502443)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4ee4aebc37\" x=\"397.237125\" y=\"22.318125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_26\">\n",
       "      <!-- 1.00 -->\n",
       "      <g transform=\"translate(404.237125 26.117344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 389.083125 185.398125 \n",
       "L 393.160125 185.398125 \n",
       "L 397.237125 185.398125 \n",
       "L 397.237125 22.318125 \n",
       "L 393.160125 22.318125 \n",
       "L 389.083125 22.318125 \n",
       "L 389.083125 185.398125 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p0d94cdd80e\">\n",
       "   <rect x=\"40.603125\" y=\"22.318125\" width=\"326.16\" height=\"163.08\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pf762f80954\">\n",
       "   <rect x=\"389.083125\" y=\"22.318125\" width=\"8.154\" height=\"163.08\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 576x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "encod_block = PositionalEncoding(d_model=48, max_len=96)\n",
    "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\n",
    "pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\n",
    "fig.colorbar(pos, ax=ax)\n",
    "ax.set_xlabel(\"Position in sequence\")\n",
    "ax.set_ylabel(\"Hidden dimension\")\n",
    "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
    "ax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\n",
    "ax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see the sine and cosine waves with different wavelengths that encode the position in the hidden dimensions. Specifically, we can look at the sine/cosine wave for each hidden dimension separately, to get a better intuition of the pattern. Below we visualize the positional encoding for the hidden dimensions $1$, $2$, $3$ and $4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"721.920312pt\" height=\"279.814375pt\" viewBox=\"0 0 721.920312 279.814375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-04-08T00:12:04.272681</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 279.814375 \n",
       "L 721.920312 279.814375 \n",
       "L 721.920312 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 45.120313 99.975268 \n",
       "L 349.483949 99.975268 \n",
       "L 349.483949 22.318125 \n",
       "L 45.120313 22.318125 \n",
       "z\n",
       "\" style=\"fill: #eaeaf2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 58.955023 99.975268 \n",
       "L 58.955023 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(55.773773 117.073705)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <path d=\"M 77.401304 99.975268 \n",
       "L 77.401304 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(74.220054 117.073705)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 95.847585 99.975268 \n",
       "L 95.847585 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 3 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(92.666335 117.073705)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <path d=\"M 114.293866 99.975268 \n",
       "L 114.293866 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 4 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(111.112616 117.073705)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 132.740147 99.975268 \n",
       "L 132.740147 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 5 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(129.558897 117.073705)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <path d=\"M 151.186428 99.975268 \n",
       "L 151.186428 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 6 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(148.005178 117.073705)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 169.632709 99.975268 \n",
       "L 169.632709 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 7 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(166.451459 117.073705)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <path d=\"M 188.07899 99.975268 \n",
       "L 188.07899 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 8 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(184.89774 117.073705)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 206.525271 99.975268 \n",
       "L 206.525271 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 9 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(203.344021 117.073705)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <path d=\"M 224.971552 99.975268 \n",
       "L 224.971552 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 10 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(218.609052 117.073705)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_11\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 243.417833 99.975268 \n",
       "L 243.417833 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 11 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(237.055333 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_12\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <path d=\"M 261.864114 99.975268 \n",
       "L 261.864114 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 12 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(255.501614 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_13\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 280.310395 99.975268 \n",
       "L 280.310395 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 13 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(273.947895 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_14\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <path d=\"M 298.756676 99.975268 \n",
       "L 298.756676 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 14 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(292.394176 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_15\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 317.202957 99.975268 \n",
       "L 317.202957 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 15 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(310.840457 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_16\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <path d=\"M 335.649238 99.975268 \n",
       "L 335.649238 22.318125 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 16 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(329.286738 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- Position in sequence -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(146.006818 130.75183)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \n",
       "L 1259 2394 \n",
       "L 2053 2394 \n",
       "Q 2494 2394 2734 2622 \n",
       "Q 2975 2850 2975 3272 \n",
       "Q 2975 3691 2734 3919 \n",
       "Q 2494 4147 2053 4147 \n",
       "L 1259 4147 \n",
       "z\n",
       "M 628 4666 \n",
       "L 2053 4666 \n",
       "Q 2838 4666 3239 4311 \n",
       "Q 3641 3956 3641 3272 \n",
       "Q 3641 2581 3239 2228 \n",
       "Q 2838 1875 2053 1875 \n",
       "L 1259 1875 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 45.120313 93.503839 \n",
       "L 349.483949 93.503839 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- −1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(20.878125 97.303058)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <path d=\"M 45.120313 61.146696 \n",
       "L 349.483949 61.146696 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(29.257813 64.945915)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 45.120313 28.789554 \n",
       "L 349.483949 28.789554 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(29.257813 32.588772)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_21\">\n",
       "     <!-- Positional encoding -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(14.798438 109.613103)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_20\">\n",
       "    <path d=\"M 58.955023 61.146696 \n",
       "L 77.401304 33.9191 \n",
       "L 95.847585 31.72443 \n",
       "L 114.293866 56.580456 \n",
       "L 132.740147 85.634663 \n",
       "L 151.186428 92.174747 \n",
       "L 169.632709 70.187783 \n",
       "L 188.07899 39.888487 \n",
       "L 206.525271 29.13389 \n",
       "L 224.971552 47.811719 \n",
       "L 243.417833 78.749666 \n",
       "L 261.864114 93.503523 \n",
       "L 280.310395 78.508663 \n",
       "L 298.756676 47.551292 \n",
       "L 317.202957 29.093472 \n",
       "L 335.649238 40.105239 \n",
       "\" clip-path=\"url(#p3f5f66f7d1)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: round\"/>\n",
       "    <defs>\n",
       "     <path id=\"m3051c0d215\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #000000\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p3f5f66f7d1)\">\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"58.955023\" y=\"61.146696\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"77.401304\" y=\"33.9191\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"95.847585\" y=\"31.72443\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"114.293866\" y=\"56.580456\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"132.740147\" y=\"85.634663\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"151.186428\" y=\"92.174747\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"169.632709\" y=\"70.187783\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"188.07899\" y=\"39.888487\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"206.525271\" y=\"29.13389\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"224.971552\" y=\"47.811719\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"243.417833\" y=\"78.749666\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"261.864114\" y=\"93.503523\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"280.310395\" y=\"78.508663\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"298.756676\" y=\"47.551292\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"317.202957\" y=\"29.093472\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3051c0d215\" x=\"335.649238\" y=\"40.105239\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 45.120313 99.975268 \n",
       "L 45.120313 22.318125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 349.483949 99.975268 \n",
       "L 349.483949 22.318125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 45.120313 99.975268 \n",
       "L 349.483949 99.975268 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 45.120313 22.318125 \n",
       "L 349.483949 22.318125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_22\">\n",
       "    <!-- Encoding in hidden dimension 1 -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(101.073381 16.318125)scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-45\" d=\"M 628 4666 \n",
       "L 3578 4666 \n",
       "L 3578 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2753 \n",
       "L 3481 2753 \n",
       "L 3481 2222 \n",
       "L 1259 2222 \n",
       "L 1259 531 \n",
       "L 3634 531 \n",
       "L 3634 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"126.5625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"181.542969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"242.724609\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"306.201172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"333.984375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"397.363281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"460.839844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"492.626953\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"520.410156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"583.789062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"615.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"678.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"706.738281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"770.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"833.691406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"895.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"958.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"990.380859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1053.857422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1081.640625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1179.052734\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1240.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1303.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1356.054688\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1383.837891\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1445.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1508.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" x=\"1540.185547\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 410.356676 99.975268 \n",
       "L 714.720312 99.975268 \n",
       "L 714.720312 22.318125 \n",
       "L 410.356676 22.318125 \n",
       "z\n",
       "\" style=\"fill: #eaeaf2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_3\">\n",
       "    <g id=\"xtick_17\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 424.191387 99.975268 \n",
       "L 424.191387 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(421.010137 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_18\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <path d=\"M 442.637668 99.975268 \n",
       "L 442.637668 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- 2 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(439.456418 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_19\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path d=\"M 461.083949 99.975268 \n",
       "L 461.083949 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- 3 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(457.902699 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_20\">\n",
       "     <g id=\"line2d_24\">\n",
       "      <path d=\"M 479.53023 99.975268 \n",
       "L 479.53023 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_26\">\n",
       "      <!-- 4 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(476.34898 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_21\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <path d=\"M 497.976511 99.975268 \n",
       "L 497.976511 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_27\">\n",
       "      <!-- 5 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(494.795261 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_22\">\n",
       "     <g id=\"line2d_26\">\n",
       "      <path d=\"M 516.422792 99.975268 \n",
       "L 516.422792 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_28\">\n",
       "      <!-- 6 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(513.241542 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_23\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <path d=\"M 534.869073 99.975268 \n",
       "L 534.869073 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_29\">\n",
       "      <!-- 7 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(531.687823 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_24\">\n",
       "     <g id=\"line2d_28\">\n",
       "      <path d=\"M 553.315354 99.975268 \n",
       "L 553.315354 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_30\">\n",
       "      <!-- 8 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(550.134104 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_25\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <path d=\"M 571.761635 99.975268 \n",
       "L 571.761635 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_31\">\n",
       "      <!-- 9 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(568.580385 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_26\">\n",
       "     <g id=\"line2d_30\">\n",
       "      <path d=\"M 590.207916 99.975268 \n",
       "L 590.207916 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_32\">\n",
       "      <!-- 10 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(583.845416 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_27\">\n",
       "     <g id=\"line2d_31\">\n",
       "      <path d=\"M 608.654197 99.975268 \n",
       "L 608.654197 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_33\">\n",
       "      <!-- 11 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(602.291697 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_28\">\n",
       "     <g id=\"line2d_32\">\n",
       "      <path d=\"M 627.100478 99.975268 \n",
       "L 627.100478 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_34\">\n",
       "      <!-- 12 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(620.737978 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_29\">\n",
       "     <g id=\"line2d_33\">\n",
       "      <path d=\"M 645.546759 99.975268 \n",
       "L 645.546759 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_35\">\n",
       "      <!-- 13 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(639.184259 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_30\">\n",
       "     <g id=\"line2d_34\">\n",
       "      <path d=\"M 663.99304 99.975268 \n",
       "L 663.99304 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_36\">\n",
       "      <!-- 14 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(657.63054 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_31\">\n",
       "     <g id=\"line2d_35\">\n",
       "      <path d=\"M 682.439321 99.975268 \n",
       "L 682.439321 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_37\">\n",
       "      <!-- 15 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(676.076821 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_32\">\n",
       "     <g id=\"line2d_36\">\n",
       "      <path d=\"M 700.885602 99.975268 \n",
       "L 700.885602 22.318125 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_38\">\n",
       "      <!-- 16 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(694.523102 117.073705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_39\">\n",
       "     <!-- Position in sequence -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(511.243182 130.75183)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_37\">\n",
       "      <path d=\"M 410.356676 93.503839 \n",
       "L 714.720312 93.503839 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_40\">\n",
       "      <!-- −1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(386.114489 97.303058)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_38\">\n",
       "      <path d=\"M 410.356676 61.146696 \n",
       "L 714.720312 61.146696 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_41\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(394.494176 64.945915)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_39\">\n",
       "      <path d=\"M 410.356676 28.789554 \n",
       "L 714.720312 28.789554 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_42\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(394.494176 32.588772)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_43\">\n",
       "     <!-- Positional encoding -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(380.034801 109.613103)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_40\">\n",
       "    <path d=\"M 424.191387 28.789554 \n",
       "L 442.637668 43.664057 \n",
       "L 461.083949 74.612019 \n",
       "L 479.53023 93.180025 \n",
       "L 497.976511 82.296736 \n",
       "L 516.422792 51.968198 \n",
       "L 534.869073 30.07833 \n",
       "L 553.315354 36.752573 \n",
       "L 571.761635 65.854662 \n",
       "L 590.207916 90.628268 \n",
       "L 608.654197 88.296653 \n",
       "L 627.100478 61.003493 \n",
       "L 645.546759 33.841994 \n",
       "L 663.99304 31.784311 \n",
       "L 682.439321 56.722271 \n",
       "L 700.885602 85.728026 \n",
       "\" clip-path=\"url(#pe2c0ad5651)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: round\"/>\n",
       "    <defs>\n",
       "     <path id=\"medc94f26df\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #000000\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pe2c0ad5651)\">\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"424.191387\" y=\"28.789554\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"442.637668\" y=\"43.664057\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"461.083949\" y=\"74.612019\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"479.53023\" y=\"93.180025\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"497.976511\" y=\"82.296736\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"516.422792\" y=\"51.968198\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"534.869073\" y=\"30.07833\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"553.315354\" y=\"36.752573\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"571.761635\" y=\"65.854662\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"590.207916\" y=\"90.628268\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"608.654197\" y=\"88.296653\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"627.100478\" y=\"61.003493\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"645.546759\" y=\"33.841994\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"663.99304\" y=\"31.784311\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"682.439321\" y=\"56.722271\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#medc94f26df\" x=\"700.885602\" y=\"85.728026\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 410.356676 99.975268 \n",
       "L 410.356676 22.318125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 714.720312 99.975268 \n",
       "L 714.720312 22.318125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 410.356676 99.975268 \n",
       "L 714.720312 99.975268 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 410.356676 22.318125 \n",
       "L 714.720312 22.318125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_44\">\n",
       "    <!-- Encoding in hidden dimension 2 -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(466.309744 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"126.5625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"181.542969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"242.724609\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"306.201172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"333.984375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"397.363281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"460.839844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"492.626953\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"520.410156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"583.789062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"615.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"678.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"706.738281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"770.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"833.691406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"895.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"958.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"990.380859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1053.857422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1081.640625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1179.052734\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1240.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1303.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1356.054688\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1383.837891\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1445.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1508.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-32\" x=\"1540.185547\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_3\">\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 45.120313 239.758125 \n",
       "L 349.483949 239.758125 \n",
       "L 349.483949 162.100982 \n",
       "L 45.120313 162.100982 \n",
       "z\n",
       "\" style=\"fill: #eaeaf2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_5\">\n",
       "    <g id=\"xtick_33\">\n",
       "     <g id=\"line2d_41\">\n",
       "      <path d=\"M 58.955023 239.758125 \n",
       "L 58.955023 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_45\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(55.773773 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_34\">\n",
       "     <g id=\"line2d_42\">\n",
       "      <path d=\"M 77.401304 239.758125 \n",
       "L 77.401304 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_46\">\n",
       "      <!-- 2 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(74.220054 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_35\">\n",
       "     <g id=\"line2d_43\">\n",
       "      <path d=\"M 95.847585 239.758125 \n",
       "L 95.847585 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_47\">\n",
       "      <!-- 3 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(92.666335 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_36\">\n",
       "     <g id=\"line2d_44\">\n",
       "      <path d=\"M 114.293866 239.758125 \n",
       "L 114.293866 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_48\">\n",
       "      <!-- 4 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(111.112616 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_37\">\n",
       "     <g id=\"line2d_45\">\n",
       "      <path d=\"M 132.740147 239.758125 \n",
       "L 132.740147 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_49\">\n",
       "      <!-- 5 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(129.558897 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_38\">\n",
       "     <g id=\"line2d_46\">\n",
       "      <path d=\"M 151.186428 239.758125 \n",
       "L 151.186428 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_50\">\n",
       "      <!-- 6 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(148.005178 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_39\">\n",
       "     <g id=\"line2d_47\">\n",
       "      <path d=\"M 169.632709 239.758125 \n",
       "L 169.632709 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_51\">\n",
       "      <!-- 7 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(166.451459 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_40\">\n",
       "     <g id=\"line2d_48\">\n",
       "      <path d=\"M 188.07899 239.758125 \n",
       "L 188.07899 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_52\">\n",
       "      <!-- 8 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(184.89774 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_41\">\n",
       "     <g id=\"line2d_49\">\n",
       "      <path d=\"M 206.525271 239.758125 \n",
       "L 206.525271 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_53\">\n",
       "      <!-- 9 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(203.344021 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_42\">\n",
       "     <g id=\"line2d_50\">\n",
       "      <path d=\"M 224.971552 239.758125 \n",
       "L 224.971552 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_54\">\n",
       "      <!-- 10 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(218.609052 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_43\">\n",
       "     <g id=\"line2d_51\">\n",
       "      <path d=\"M 243.417833 239.758125 \n",
       "L 243.417833 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_55\">\n",
       "      <!-- 11 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(237.055333 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_44\">\n",
       "     <g id=\"line2d_52\">\n",
       "      <path d=\"M 261.864114 239.758125 \n",
       "L 261.864114 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_56\">\n",
       "      <!-- 12 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(255.501614 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_45\">\n",
       "     <g id=\"line2d_53\">\n",
       "      <path d=\"M 280.310395 239.758125 \n",
       "L 280.310395 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_57\">\n",
       "      <!-- 13 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(273.947895 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_46\">\n",
       "     <g id=\"line2d_54\">\n",
       "      <path d=\"M 298.756676 239.758125 \n",
       "L 298.756676 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_58\">\n",
       "      <!-- 14 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(292.394176 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_47\">\n",
       "     <g id=\"line2d_55\">\n",
       "      <path d=\"M 317.202957 239.758125 \n",
       "L 317.202957 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_59\">\n",
       "      <!-- 15 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(310.840457 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_48\">\n",
       "     <g id=\"line2d_56\">\n",
       "      <path d=\"M 335.649238 239.758125 \n",
       "L 335.649238 162.100982 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_60\">\n",
       "      <!-- 16 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(329.286738 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_61\">\n",
       "     <!-- Position in sequence -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(146.006818 270.534688)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_6\">\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_57\">\n",
       "      <path d=\"M 45.120313 233.286696 \n",
       "L 349.483949 233.286696 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_62\">\n",
       "      <!-- −1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(20.878125 237.085915)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_58\">\n",
       "      <path d=\"M 45.120313 200.929554 \n",
       "L 349.483949 200.929554 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_63\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(29.257813 204.728772)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_59\">\n",
       "      <path d=\"M 45.120313 168.572411 \n",
       "L 349.483949 168.572411 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_64\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(29.257813 172.371629)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_65\">\n",
       "     <!-- Positional encoding -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(14.798438 249.39596)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_60\">\n",
       "    <path d=\"M 58.955023 200.929554 \n",
       "L 77.401304 180.551117 \n",
       "L 95.847585 169.27126 \n",
       "L 114.293866 172.12622 \n",
       "L 132.740147 187.841315 \n",
       "L 151.186428 209.400055 \n",
       "L 169.632709 227.17688 \n",
       "L 188.07899 233.234779 \n",
       "L 206.525271 224.869018 \n",
       "L 224.971552 205.814747 \n",
       "L 243.417833 184.57933 \n",
       "L 261.864114 170.643974 \n",
       "L 280.310395 170.230546 \n",
       "L 298.756676 183.523619 \n",
       "L 317.202957 204.588128 \n",
       "L 335.649238 224.019151 \n",
       "\" clip-path=\"url(#p3644005cad)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: round\"/>\n",
       "    <defs>\n",
       "     <path id=\"m6e82106f93\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #000000\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p3644005cad)\">\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"58.955023\" y=\"200.929554\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"77.401304\" y=\"180.551117\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"95.847585\" y=\"169.27126\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"114.293866\" y=\"172.12622\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"132.740147\" y=\"187.841315\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"151.186428\" y=\"209.400055\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"169.632709\" y=\"227.17688\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"188.07899\" y=\"233.234779\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"206.525271\" y=\"224.869018\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"224.971552\" y=\"205.814747\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"243.417833\" y=\"184.57933\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"261.864114\" y=\"170.643974\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"280.310395\" y=\"170.230546\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"298.756676\" y=\"183.523619\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"317.202957\" y=\"204.588128\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m6e82106f93\" x=\"335.649238\" y=\"224.019151\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 45.120313 239.758125 \n",
       "L 45.120313 162.100982 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 349.483949 239.758125 \n",
       "L 349.483949 162.100982 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 45.120313 239.758125 \n",
       "L 349.483949 239.758125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 45.120313 162.100982 \n",
       "L 349.483949 162.100982 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_66\">\n",
       "    <!-- Encoding in hidden dimension 3 -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(101.073381 156.100982)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"126.5625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"181.542969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"242.724609\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"306.201172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"333.984375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"397.363281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"460.839844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"492.626953\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"520.410156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"583.789062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"615.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"678.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"706.738281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"770.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"833.691406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"895.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"958.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"990.380859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1053.857422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1081.640625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1179.052734\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1240.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1303.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1356.054688\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1383.837891\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1445.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1508.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-33\" x=\"1540.185547\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_4\">\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 410.356676 239.758125 \n",
       "L 714.720312 239.758125 \n",
       "L 714.720312 162.100982 \n",
       "L 410.356676 162.100982 \n",
       "z\n",
       "\" style=\"fill: #eaeaf2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_7\">\n",
       "    <g id=\"xtick_49\">\n",
       "     <g id=\"line2d_61\">\n",
       "      <path d=\"M 424.191387 239.758125 \n",
       "L 424.191387 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_67\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(421.010137 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_50\">\n",
       "     <g id=\"line2d_62\">\n",
       "      <path d=\"M 442.637668 239.758125 \n",
       "L 442.637668 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_68\">\n",
       "      <!-- 2 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(439.456418 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_51\">\n",
       "     <g id=\"line2d_63\">\n",
       "      <path d=\"M 461.083949 239.758125 \n",
       "L 461.083949 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_69\">\n",
       "      <!-- 3 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(457.902699 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_52\">\n",
       "     <g id=\"line2d_64\">\n",
       "      <path d=\"M 479.53023 239.758125 \n",
       "L 479.53023 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_70\">\n",
       "      <!-- 4 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(476.34898 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_53\">\n",
       "     <g id=\"line2d_65\">\n",
       "      <path d=\"M 497.976511 239.758125 \n",
       "L 497.976511 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_71\">\n",
       "      <!-- 5 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(494.795261 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_54\">\n",
       "     <g id=\"line2d_66\">\n",
       "      <path d=\"M 516.422792 239.758125 \n",
       "L 516.422792 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_72\">\n",
       "      <!-- 6 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(513.241542 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_55\">\n",
       "     <g id=\"line2d_67\">\n",
       "      <path d=\"M 534.869073 239.758125 \n",
       "L 534.869073 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_73\">\n",
       "      <!-- 7 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(531.687823 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_56\">\n",
       "     <g id=\"line2d_68\">\n",
       "      <path d=\"M 553.315354 239.758125 \n",
       "L 553.315354 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_74\">\n",
       "      <!-- 8 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(550.134104 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_57\">\n",
       "     <g id=\"line2d_69\">\n",
       "      <path d=\"M 571.761635 239.758125 \n",
       "L 571.761635 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_75\">\n",
       "      <!-- 9 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(568.580385 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_58\">\n",
       "     <g id=\"line2d_70\">\n",
       "      <path d=\"M 590.207916 239.758125 \n",
       "L 590.207916 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_76\">\n",
       "      <!-- 10 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(583.845416 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_59\">\n",
       "     <g id=\"line2d_71\">\n",
       "      <path d=\"M 608.654197 239.758125 \n",
       "L 608.654197 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_77\">\n",
       "      <!-- 11 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(602.291697 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_60\">\n",
       "     <g id=\"line2d_72\">\n",
       "      <path d=\"M 627.100478 239.758125 \n",
       "L 627.100478 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_78\">\n",
       "      <!-- 12 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(620.737978 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_61\">\n",
       "     <g id=\"line2d_73\">\n",
       "      <path d=\"M 645.546759 239.758125 \n",
       "L 645.546759 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_79\">\n",
       "      <!-- 13 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(639.184259 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_62\">\n",
       "     <g id=\"line2d_74\">\n",
       "      <path d=\"M 663.99304 239.758125 \n",
       "L 663.99304 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_80\">\n",
       "      <!-- 14 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(657.63054 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_63\">\n",
       "     <g id=\"line2d_75\">\n",
       "      <path d=\"M 682.439321 239.758125 \n",
       "L 682.439321 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_81\">\n",
       "      <!-- 15 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(676.076821 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_64\">\n",
       "     <g id=\"line2d_76\">\n",
       "      <path d=\"M 700.885602 239.758125 \n",
       "L 700.885602 162.100982 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_82\">\n",
       "      <!-- 16 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(694.523102 256.856563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_83\">\n",
       "     <!-- Position in sequence -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(511.243182 270.534688)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_8\">\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_77\">\n",
       "      <path d=\"M 410.356676 233.286696 \n",
       "L 714.720312 233.286696 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_84\">\n",
       "      <!-- −1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(386.114489 237.085915)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_78\">\n",
       "      <path d=\"M 410.356676 200.929554 \n",
       "L 714.720312 200.929554 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_85\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(394.494176 204.728772)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_79\">\n",
       "      <path d=\"M 410.356676 168.572411 \n",
       "L 714.720312 168.572411 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_86\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(394.494176 172.371629)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_87\">\n",
       "     <!-- Positional encoding -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(380.034801 249.39596)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_80\">\n",
       "    <path d=\"M 424.191387 168.572411 \n",
       "L 442.637668 175.795831 \n",
       "L 461.083949 194.240974 \n",
       "L 479.53023 215.67244 \n",
       "L 497.976511 230.521489 \n",
       "L 516.422792 232.158311 \n",
       "L 534.869073 219.852094 \n",
       "L 553.315354 199.097327 \n",
       "L 571.761635 179.160615 \n",
       "L 590.207916 168.943312 \n",
       "L 608.654197 173.007253 \n",
       "L 627.100478 189.537961 \n",
       "L 645.546759 211.154797 \n",
       "L 663.99304 228.206249 \n",
       "L 682.439321 233.079196 \n",
       "L 700.885602 223.597927 \n",
       "\" clip-path=\"url(#p787a98ff76)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: round\"/>\n",
       "    <defs>\n",
       "     <path id=\"m00437e3473\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #000000\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p787a98ff76)\">\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"424.191387\" y=\"168.572411\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"442.637668\" y=\"175.795831\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"461.083949\" y=\"194.240974\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"479.53023\" y=\"215.67244\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"497.976511\" y=\"230.521489\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"516.422792\" y=\"232.158311\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"534.869073\" y=\"219.852094\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"553.315354\" y=\"199.097327\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"571.761635\" y=\"179.160615\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"590.207916\" y=\"168.943312\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"608.654197\" y=\"173.007253\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"627.100478\" y=\"189.537961\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"645.546759\" y=\"211.154797\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"663.99304\" y=\"228.206249\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"682.439321\" y=\"233.079196\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m00437e3473\" x=\"700.885602\" y=\"223.597927\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 410.356676 239.758125 \n",
       "L 410.356676 162.100982 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_19\">\n",
       "    <path d=\"M 714.720312 239.758125 \n",
       "L 714.720312 162.100982 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 410.356676 239.758125 \n",
       "L 714.720312 239.758125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 410.356676 162.100982 \n",
       "L 714.720312 162.100982 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_88\">\n",
       "    <!-- Encoding in hidden dimension 4 -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(466.309744 156.100982)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"126.5625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"181.542969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"242.724609\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"306.201172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"333.984375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"397.363281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"460.839844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"492.626953\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"520.410156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"583.789062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"615.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"678.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"706.738281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"770.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"833.691406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"895.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"958.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"990.380859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1053.857422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1081.640625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1179.052734\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1240.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1303.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1356.054688\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1383.837891\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1445.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1508.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-34\" x=\"1540.185547\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p3f5f66f7d1\">\n",
       "   <rect x=\"45.120313\" y=\"22.318125\" width=\"304.363636\" height=\"77.657143\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pe2c0ad5651\">\n",
       "   <rect x=\"410.356676\" y=\"22.318125\" width=\"304.363636\" height=\"77.657143\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p3644005cad\">\n",
       "   <rect x=\"45.120313\" y=\"162.100982\" width=\"304.363636\" height=\"77.657143\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p787a98ff76\">\n",
       "   <rect x=\"410.356676\" y=\"162.100982\" width=\"304.363636\" height=\"77.657143\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 864x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_theme()\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12,4))\n",
    "ax = [a for a_list in ax for a in a_list]\n",
    "for i in range(len(ax)):\n",
    "    ax[i].plot(np.arange(1,17), pe[i,:16], color=f'C{i}', marker=\"o\", markersize=6, markeredgecolor=\"black\")\n",
    "    ax[i].set_title(f\"Encoding in hidden dimension {i+1}\")\n",
    "    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n",
    "    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n",
    "    ax[i].set_xticks(np.arange(1,17))\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=10)\n",
    "    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n",
    "    ax[i].set_ylim(-1.2, 1.2)\n",
    "fig.subplots_adjust(hspace=0.8)\n",
    "sns.reset_orig()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the patterns between the hidden dimension $1$ and $2$ only differ in the starting angle. The wavelength is $2\\pi$, hence the repetition after position $6$. The hidden dimensions $2$ and $3$ have about twice the wavelength. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate warm-up\n",
    "\n",
    "One commonly used technique for training a Transformer is learning rate warm-up. This means that we gradually increase the learning rate from 0 on to our originally specified learning rate in the first few iterations. Thus, we slowly start learning instead of taking very large steps from the beginning. In fact, training a deep Transformer without learning rate warm-up can make the model diverge and achieve a much worse performance on training and testing. Take for instance the following plot by [Liu et al. (2019)](https://arxiv.org/pdf/1908.03265.pdf) comparing Adam-vanilla (i.e. Adam without warm-up) vs Adam with a warm-up:\n",
    "\n",
    "<center width=\"100%\"><img src=\"warmup_loss_plot.svg\" width=\"350px\"></center>\n",
    "\n",
    "Clearly, the warm-up is a crucial hyperparameter in the Transformer architecture. Why is it so important? There are currently two common explanations. Firstly, Adam uses the bias correction factors which however can lead to a higher variance in the adaptive learning rate during the first iterations. Improved optimizers like [RAdam](https://arxiv.org/abs/1908.03265) have been shown to overcome this issue, not requiring warm-up for training Transformers. Secondly, the iteratively applied Layer Normalization across layers can lead to very high gradients during the first iterations, which can be solved by using [Pre-Layer Normalization](https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf) (similar to Pre-Activation ResNet), or replacing Layer Normalization by other techniques ([Adaptive Normalization](https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf), [Power Normalization](https://arxiv.org/abs/2003.07845)). \n",
    "\n",
    "Nevertheless, many applications and papers still use the original Transformer architecture with Adam, because warm-up is a simple, yet effective way of solving the gradient problem in the first iterations. There are many different schedulers we could use. For instance, the original Transformer paper used an exponential decay scheduler with a warm-up. However, the currently most popular scheduler is the cosine warm-up scheduler, which combines warm-up with a cosine-shaped learning rate decay. We can implement it below, and visualize the learning rate factor over epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "    \n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "JVBERi0xLjQKJazcIKu6CjEgMCBvYmoKPDwgL1BhZ2VzIDIgMCBSIC9UeXBlIC9DYXRhbG9nID4+CmVuZG9iago4IDAgb2JqCjw8IC9FeHRHU3RhdGUgNCAwIFIgL0ZvbnQgMyAwIFIgL1BhdHRlcm4gNSAwIFIKL1Byb2NTZXQgWyAvUERGIC9UZXh0IC9JbWFnZUIgL0ltYWdlQyAvSW1hZ2VJIF0gL1NoYWRpbmcgNiAwIFIKL1hPYmplY3QgNyAwIFIgPj4KZW5kb2JqCjExIDAgb2JqCjw8IC9Bbm5vdHMgMTAgMCBSIC9Db250ZW50cyA5IDAgUiAvTWVkaWFCb3ggWyAwIDAgNTAwLjg3ODEyNSAyMjcuMDQyNSBdCi9QYXJlbnQgMiAwIFIgL1Jlc291cmNlcyA4IDAgUiAvVHlwZSAvUGFnZSA+PgplbmRvYmoKOSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDEyIDAgUiA+PgpzdHJlYW0KeJy9WE1zGzcMve+v4DE5hMYHAZLHJG09zfSSxDM9ND24jpI4YzuN7bR/vw9UbGnV2KM2kj0jj/YNBeIBDyC4Bz8s/jo9Wbw6fJaev54OVk8nVxOnj9PBU07vrxL+JUof8fkb3w/jeaL0hNL5ZES51cZi8Xy2/ixSMxV8OcPitYcP0/Ruoty5eqlkraXNh9KJu1Nt6TI2PvzXgtuHaWP1NJWaZbl/kUwyvp1PpWv2TfhsHRayzHSD3xqZwcPzz+lbW5TiuSR2zQQ/FunXdJEOnsoyei/w+YjPiN5GmL1m805aZv6u0JkD0+vpZfp8Y5gyGzJzY3s8Hn5Fp89I4MgRJy/ZyE0LdepJSsb3sHdyPj07Sgc/cWJOR8gJfnD0dvotPaLH6fd09GL68Wh6OXbcKWXmllXYm844r8E7IM3UM2vhplykbsVabM+8vWeqRVXmvFfwLnhbz60hiMK92Fa8UbN75Q1PchPvjWe81+Ad8Bam7NoLjBTXrXjXPedbKmWr3ecyX6G7YG1RMzBGMuK4BWumPacbyssQs9Q+470G74C4oryphTUtfbumxvuub62w0UqXNie+gndBPOpbw1oh3q6v8b4LvIjAp2q1zg/dFbwD4gUFbi2sFdPtGhvvu8JL1WydIMM58RW8C+KOwimw5ka2XWeTjRqf9UnJtbeCvKSe66YhmRv6+XpxeXx9+uniKr15dHqR/ji+PvmwuHrzeM9hvTXWcynUTDZmuFv4fzdOycXNw5ZhOlI1ZAv5umcmyvvW0o2x2vHNMLrMSa/g7yddDb24Yo7GpHA/aXkg0mNeo94x/c1Yr+HfT5ujGqsUNFBcIO7lXR6KtzZcYYqVvsF7he+At1omqoSrkpPfy9sfire3XKFA3riereE74I2rTyvsHM/3824Pxbu37Ky+UdwreAesO46I6txI7P7zMW+cEmHnSVhkyxis4oyonM3uPSN+WRxfXpxevE84KRbp3fHJ9afLXQUzW/obd3wMt6Rd2CqIl+XVXzAEQK43l/+SXh2medDX79Zr50irGb8nA5POMUsh9og64FaJOlzqFXZNB4xYdnRdj7A6KlKw4GxiipmOmgaMWFXiYgPvWRs8rSMLFVdP9Ffg7DBZDSYZSZfWMccFjtNcqGGCDV/MS6txprFyrkLaJHHrEIYoDzMKHlpqw7bwV6ubcOCIL06wIMItJF91eGmWSxWXMAOHEWtUAXDEHDcQQjhxAoB2XVrBoIoB3QjLcUAUwTVlmGmcBY0I4uSQA/qIDjMdExlkYlFoSHCvZVQtpo9shUAXOFTZaxtBEIbggIpDXxUCdhpjkUC67p0VMFYUxHUsR9PyUDG2LRp6NImYCRbBGQfFZV/D6R2ZQqgzkk2RKlwtcZ6ThPsKd1AWHEEmR3BUR64UIhck3zmh+KoLRehVaqx27Nolc1wpx2K04+hKnVODbF3bOIcxiGeGIBUwvBJEcqzGtaxBBiVVaEa9L1G0NrhU2RMi3R0aGDtiTZzryLwjvehWkQxt8K6RI+/umZu1sRbiwdxPhRNyiArlOBaBZEXirSRHJyMkoA8YdySVhjTDH8MWGjawA4QJWSTzDJVjvgw0lGgNgkoGBSEFw7KGKisinCz266ihAYdqLNJtCBLDyrBRwlPMFTDNgBnqCNiG4qGRZASdItPDCHyN6CJ5Jd4E4ErTBlyR3h6vqgCbKKbd8UoO+6M6ELxlMeNwvBuet1CJdgJ2X19f3jlszkb4b74BhN1vvko8v/NVYvziv7ySnK1fs3TvDgSC278ggsCXVlCPX63c3d2ff7o6vUBzfpwUuremy7/06Pjy/MmXP9Nt938V3f81bgdvv5wt5gfA9A/PRrIACmVuZHN0cmVhbQplbmRvYmoKMTIgMCBvYmoKMTM5MgplbmRvYmoKMTAgMCBvYmoKWyBdCmVuZG9iagoxNyAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDE4ID4+CnN0cmVhbQp4nDMyt1AwgMMUQ640AB46A1cKZW5kc3RyZWFtCmVuZG9iagoxOCAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDE1MiA+PgpzdHJlYW0KeJxFTzkOwzAM2/0KfiCAdcvvSRF0SP+/VnJQdBJNmaQoIphwwsFzgVcgOPGiwaIP9xmUhoM0QVpcBIgIinMUTG8xBfVgW8UWEHGEQGliJTS5aOKAudZfhqujAzrrHL/Ue7AF3Gft2q8MacFSWr0KSZ1QL120XVS3ryUki7HK5bnnc0gjmozWHia73a50/tvd4z2uL8iYMMUKZW5kc3RyZWFtCmVuZG9iagoxOSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDE1MiA+PgpzdHJlYW0KeJw9T0kOAzEMuucVfGAkGzuJ5z1TVT20/7+WpMsJBMGQiIBhOA7aCZ4Tk4WbN2d+tFf7uc/mvXD0AG3CjRvZT1xtsaBJMUQ5vAYyXY7PRNYJ17seAx7C4nI41Gy6pAVy1bPKr929mAorMXKAVGjkLssdppxUeE0OnV34nSLm08EyaAUVzyU7ccxlCT0L/x9e7dHub5ayMYcKZW5kc3RyZWFtCmVuZG9iagoyMCAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDUyID4+CnN0cmVhbQp4nDM2NlYwUDA2UjAyNFUwNjACYmOFFEMuqEguiAESyuGCSUJYIMkcmKocrgyuNAD+SQ4FCmVuZHN0cmVhbQplbmRvYmoKMjEgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCA0NCA+PgpzdHJlYW0KeJwzMrdQMFCwNAQShkDS0MBAIcWQC8zP5YIK5HAZorBANJTK4EoDAJdwDIQKZW5kc3RyZWFtCmVuZG9iagoyMiAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDI3NSA+PgpzdHJlYW0KeJw1UUtuBTEI288pfIFK/EnO86qnbnr/bU3SJ80IErAxTmZBEIYvVaQstG5868MbT8fvJOHNEr9ELWQ2Xs9iLhtKVAVj8NxT0N5odpr54bLOE1+P673xaEaFd6F2shISRG/KWCjSBzuKOStVyM3KoroKxDakGSspFLbkaA7OmjiKp7JgRQxxJsouo7592BKb9L6RRFGlywhrBde1PiaM4Imvx+RmmvyduxpV8Z4sajqmmc7w/7k/j/rHtcnM8/ii3Eh78OuQCriqOVcWDjthzDmJx5rqWHPbx5ohCJ6GcOIdN1lQ+XRkXEyuwQxJWeFwRt0hjBzufm9oSxmfjU+W5wmUlufZk7a24LPKrPX+A5pDZi0KZW5kc3RyZWFtCmVuZG9iagoyMyAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDExNiA+PgpzdHJlYW0KeJw1TjkOA0EM6v0KnuDb4/dsFG0x+X8b7yhpDMKAiEgwhHuulaGk8RJ6KONDumJwH4w8LA3hDLVRxqws8G5cJFnwaoglPP2UevjzGRbWk5ZY06MnFf20LKTaeLQcGQFjRq6CSZ4xF/1n7d+qTTe9v3LSItAKZW5kc3RyZWFtCmVuZG9iagoyNCAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDI2OSA+PgpzdHJlYW0KeJw1UcttxTAMu3sKjmD97XleUfSQ7n8tpaBAHCrRj6QjEht6+YptKLn4ktXhcfxOsPEs2wOsU4EZXPpJwWeF4bJRIeq4B8KJn9UfcgqSBlUe4clgRi8n6IG5wYpYPat7jN0ePVzh5wyGKjMTca7dizjEci7f3eMXaQ6TQnpC60XusXj/bBIlZalE7tPcgmIPCVshvF7cs4cBVz0tKuqiWyhdSC9zZJFEcaCKjFfaRcQmUhM5ByVpuhPHIOeqpAW9IjhxUJt8R047/CacRjk9d4shwsyusaNNcqVoP2PSHbEWzu2BtlPHJDWaz1rdtJ61ci6ldUZoV2uQpOhNPaF9vZ//e37Wz/r+A+1NYUoKZW5kc3RyZWFtCmVuZG9iagoyNSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDk0ID4+CnN0cmVhbQp4nE2NQQ7AIAgE77yCJ7hQtP6naTzY/18rGKMXmOwurFnmxNAxLN1ckPkBqbjwxUYBd8IBYjJAtUa80wUcNF1/tmmeursp+Y/o6dSCPD87rdhQa11VskobvT+6wSINCmVuZHN0cmVhbQplbmRvYmoKMjYgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAyNDkgPj4Kc3RyZWFtCnicRVFJbsQwDLv7FfxAAWu1/Z4pBj1M/38tmQToIRFjSyTFVDUm0vFlhrLGmo1vGzyxvfE7LBJ2Cp9hOWGlp2HstG04iddwjiyDR6MnnJDlNcJCIPJgNWId2Nw8T77FlR7k8Kt6lG6EdkEd4YnYHK8QVzm/+FghzqLIvCrF6fQ6oaM4dHeCWrox9TTdazZvzXA5qIWIrZX8XvgzkuT/qN11S9oH1UbGJPJpSG2ZjVwFp5yqLNaFZD5pOoudpiCSKUX3FW88MXtqLSFb7KeSUSmLWV1JMDujS3LoxyhT1TtrIaMCZ4wzIuKqzDfFsvD8u9f4Ge8/0LZZaAplbmRzdHJlYW0KZW5kb2JqCjI3IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMzU4ID4+CnN0cmVhbQp4nD1SS24FMQjb5xRcoFL4k/NMVXXRd/9tDenryoiJMTbjHrRJiz6YyXlT8qFPXnaK3Jhey9B0NfpZtoU8ivTg6VHSTIp96FnqSqHoCNCCpM7gsyT4djTwokjYKfDqWVzNVuII8gR663h/gZqdIBYnww6NGq3DmGQbnRQyMRLwzXbrQN3gRQKcwJdzBnu3nMo20MCzdtDTDFsqOG1b9x4UFXzpqvdzdNkwsaAJPjjtp8iwqJ67ywQQiQTh/0yQUjGIvVimYm+HM2ScRNsSmkS4Qcc6CsvO8kbChrJl2Qs8DOaaC8mxwbZ3b6YnKTsOBBHJsyqO0EseWEOc75M+6xsRn7H6uhUO2zZ5zlBTQzNhnhNBFIHeTkomapwwSRzjEVh5AxYR7qJ/hUQ4BfLuMbZxSVBM0MmLIpNlV9kXDVK+HLV7M8PfhXiks4FWXYS4/XV2zQv+57DLTBlDWfS22Ha/fgGL6IoVCmVuZHN0cmVhbQplbmRvYmoKMjggMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAxNDEgPj4Kc3RyZWFtCnicNY/LDcMwDEPvnoIj6GvJ86Qockj3v5ZOkItNkcYTnTkhiOKRqigpfHRwnmb4bbGta7zho6Y3VcxE9kLoQlrAKxEROIa7wGfAVsJaYaXQVUwsHeoFCwNNI0ho693g4t1gI80FJVFbYLKJJnzcJLqS/BDDc+9in5QFJznp+uq7/PH+4hrn+P4BvfcriAplbmRzdHJlYW0KZW5kb2JqCjI5IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggNDE5ID4+CnN0cmVhbQp4nD1SW24EMQj7n1NwgUrhnZxnq6o/e//f2sxspc3CBAK2IbNkSah8qUqqSeuRb720W3xveV8aiC8VVZewJSclIuV1ISPqCH5xxqQHrunskt1SdkQtpYrpWi6NOoY6bGKdY1+Xe4/Hfr3QzQpvWCvwX7YltqNo3NaNEXhxEOkYFJH9wAo/gzOIF/38YYKI8Qv5GeKpeIvIIEh0NSCmABbntovV6GmwF5gbWjCJtZYLEEeNcNa3fV18RU9jI674mvSyec37oLHVLAInwQjNEEUNN7KGmp4p6g64JfpP4PfSpMzNsdADCG1QhZTK+snnpmjhJIIbg+WgjKI5gNFz35PhtZ43vm2q+AEcinY+Qo+HMfjGfhxE0Lcg7T22crxZuIEQFIEWCNB5boCEGcRWyj5Em/ga9NXy4TPc/NblPZ6inzozcDASneXS4iIusN4U1BZk4wBt1gxqLgEnMoYh4UPHIXL7UNC1Znobm3nLovXItGbj6AE6M2zjKc+i+J4UDjNSnGSTGIvmlBKeYh+Zoa0jCuBi2jZEQA2r86FIuj9/mtOljAplbmRzdHJlYW0KZW5kb2JqCjMwIDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjk1ID4+CnN0cmVhbQp4nCVSSY4EMQi75xX+wEhhJ++pUWsO0/+/tkkfShAgxnaqVLERGz8iyDaUNn5lRTc0BO81RdXC/8o0iB8kT30QFeDpWaGGA88viHXe+Czb52bqCYechgXLFyIhovwEXZCa8VSoyXDRExOM+4i+4VZIZ0lxFBE9KLqRQtgk70go8+zBV4KXGEz9qjFKmfgsj3OzEEaSi2xkzZIUbpgtlBwUkEVTuIHrq6iyKT8oMMgmLDlTpBNkMMydGEltbuTgdS2YSAts30ybHaORzHNPR4VILRB2woM6yGcPmoTDg65w1ok/0Sh/nGNGE2T00kUph+7bOVPlk1GZULfOfU5yz3E0vRxqB1Y9gZTpNBOvxBCgXSJjXgws6UTNi40Z3//gWX/r9QHuVmfzCmVuZHN0cmVhbQplbmRvYmoKMzEgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCA1MCA+PgpzdHJlYW0KeJwzMrdQMFCwNAYShhYWCuaGZgophlxgfi6IAgnkcMGkICwDIA1WkcOVwZUGAKAvDMUKZW5kc3RyZWFtCmVuZG9iagozMiAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDU5ID4+CnN0cmVhbQp4nDM1NVMwUDA3BhKmRoYK5oZmCimGXGB+LogCCeRwGZpZILEsTIAMkGo4wwBIg/XkcGVwpQEAnlEQIwplbmRzdHJlYW0KZW5kb2JqCjMzIDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjgxID4+CnN0cmVhbQp4nD2Ru3EDMQxE86sCJYD4kvXI41Fw7j/1Lk6jQMIeF59HsM1EpQ//9Eivkp91zfcfAw/uy099VJSOSt1TkLGk4L6u7JDaLqU4QVYZom04hZxETiUiHUa4cKCijpSHhNXU+jF2qy2eLbla3FMijrhxTmCOL+QfF1cDSYkdZQ26WMXMMHfJnbJOwiGpgj6RxxigWejI+2zkgCJSzEjRYtGcgxprBY+L7RLHNmy6eSUmgyLhLvy8hgiOwcF2XEG9Nup84uta/ah7FBn6QXrPQSj37d2jgGU6KhJIGBW9JZbzkrhy4NG4wMBgXjpyHiAKeThJ9Ds98VnzKHTKUCwFtYurJnAcPhLq0vAIz3TGB5jq/vKR9fcfFT1pOwplbmRzdHJlYW0KZW5kb2JqCjM0IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggNDQyID4+CnN0cmVhbQp4nC2TS24EMQhE930KLhDJfG2fZ6Ioi+T+2zyYrKBtXFQVdNWWJZHyoSqlKduOfOrDifmS30c9yI78kB2xZaJxRDexgvzK69F9Ra24ddFFlZfc4MLqyFniC1wXj+rwevxYJ6Eu+0iEymmYqCv3SvChGpKN5l2fWqKpkubTeeI06My47Yru2m8s+03cJQZgg9qhmRNv9o01I+jQ2+njauJohS4Em6QtKkrpxe3uNxqcXB9egUt3D+3Xg7rAQkjFnZDex51wjM4CS+lWbo0D3ypw4FhnowILVvM17rfCcdfMwC0m4tbek0WfaCsirkbLFaAkPQvUJZlHynbfwCvBzotrWe9cxzFe5trMswlnQvxQc4olgJOucSjhi0P4VmyC070jLPeazOxSETPxUp8dAO2+t6KlT/TmMpm2m0yBqSicYs96LNTkZmJtJw6DGNbKgvpAXWt19iFuz2uUUePUZ8ELjol+X41WbJmB21tsoE0cnzprFkWdoiWZy3t9MK19a79pdljs6u6x5QNjls5v4bkmQvi8fxRjyBRoT7BmPXq7mXO7IqezLk8W2XtgVJr8/1Kv5/v5+gPKV6S7CmVuZHN0cmVhbQplbmRvYmoKMzUgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAxNjcgPj4Kc3RyZWFtCnicNVC5EQMhEMupQiWwP1uPPR4H5/5Ti3sSJLQjraDdMSHKo81QknjL0LmF36C+lWPI7JupLKjXZkZjF29E32pD5sRraBakElqc6oQugZpy4lJ3TpjcLM1h4ZtFQ9ORtf2NXE9i9uKOoOPaWRrQSE6WPYlP/2OUJeszj46N3o4gJ2OTaIEXt0gTFam9m3FvdMKjELmIcnr4mu4z5fqUY3zH5w9upjrSCmVuZHN0cmVhbQplbmRvYmoKMzYgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCA0NDEgPj4Kc3RyZWFtCnicNZJJkh0xCET3dQou0BFi0HSe73B40b7/1i+p9qIKJARkJsy5bFgu+3K36WUzh/3yp0bZKvv75HaFs4Y5xodN+zxxhn1Ni9qdGJ5tP4/Pt5R7WNgJo9znmdQ+KnNTf8/NpZwVVjw+k74WY3G9KBvbaBBVdq/F1Gv3bbEuucdi306NowTnFJfng8xbpOGTRweA5Ni0pC35efmiI/Lo/Nrz2hn/I4ebc4FG3k6rOIrMYaW36FBTKKItakCyb4YsQgG+srEtvIBhod2dzTznfSWRtN8PpwKjihGERy1J5uNYoZ9n2hwSfzMfIYyBmvHy1LSi1VOOuMlLNNSLRG7N9PMIw2SkBee6fBN/a5JF3RKGDSsq1iHqwl6HN2KEyq2CbHY1vEDP7/Y8JzEmVl16CWPBVfAGQxqNYTSKwJIFD4fekCj2s2qf50+LH9Bn7da7XRpbIGVoP0KLoMYhSa/2DkkBHuO22NyMNNcIoO6lNr2VwPZ1gEoE6m2zc+SpCmt14cL6npZ/NyhNdApBWW9hUETnexRNNN73ZzXYvNwqhj1q3hO5QICQiDkb1QTfbfqh+g3t3/8AxuunNwplbmRzdHJlYW0KZW5kb2JqCjM3IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjU0ID4+CnN0cmVhbQp4nE2RS27EQAhE930KLhCJfzfncRRl4dx/mwLPSLOwKFE08HBEElMGfYlQSNCWpG9Z4hv5vwWH6e7Qxr0k7EN5tinFFJyktimMSSsmXsv0yVggqpGdxBCB4+LkdcgN32Fy7OGogrPhGHqfIudAVZFthRMMZTVrGvtEzX7TSjY6lZGgyr0oZwwaCOaGDqCWTLyWao2Sg8+pccF4vbjv9QsFojTcYBSjLlCIPWVD6+lq7HYM4Nnj9Pgz1VRbWOBqG9BY6gXdw4Cn2KqeqNmHatXH6gqbNwAHCHoFsBxZ7QMdDDkTgQDIyeB3dIW0w7MWetpuqpx3b5QLWD//cgJc6gplbmRzdHJlYW0KZW5kb2JqCjM4IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjQ4ID4+CnN0cmVhbQp4nDVRS24FMQjb5xS+QKWASUjOM9VTF+39tzW8djEyCcEfZs2JCV58mCGuYXHi00bMgN2Jn1GXlhffI44qu4iVSEfYqcFnUN0F0prEczU+wye7stgwh+m4ju73VB01a9naLkLRXNCIEOt27ER5eMZZiCKoViZslc+isSNZ2XE5LtclXCgmvnNQ75dpvmlLI6Ls6/vzH8eltls9wUXFpHip18zoSS4hrXnFIwZOTSqK521UVEZXJmcR3sHCyovpxFTHNedv9N0dVbXiemG1jK1vdrK7kLuD7VpoFEheTRWk1i8QyfW6PuztUNq16v9f94yv8foFgJNZPwplbmRzdHJlYW0KZW5kb2JqCjM5IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjYyID4+CnN0cmVhbQp4nDVQMY4EMQjr8wo+cFLAQJL37Gl1xez/2zNEW9lDxmA7ImUKXH5UxbfL0pRfHT6N809jhjwDeURdYNq/WqzG1zCNZroWFeoQA8c6t3jIVuE8TVQ3p3zV2HXPZjTE4ZgEOsVNwL1JQ6fGVLpz84T4clHw+2QtXyrBhUZRYHILGumGEYpBTYJGQE1ovSAZ8CzBrqB1Immwr5NV7Gd8C7hsFnYPz/gbatFuP830MBI28xIzGa9u6PGKa8YQ7IjFZVUEBiABeCcFyRLm7sMsDEd8MtxejRalKAZjHfwDjF4avxpElwIac1ZpeZHR7TKlxtuWi19bNIrFFxoHvX2jvBjr/Q9ual9kCmVuZHN0cmVhbQplbmRvYmoKNDAgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAyNTkgPj4Kc3RyZWFtCnicNVDJbQQxDPu7CjYQQKePeiZY5LHp/xtK3mA8EGFLpMjMCYEvfKkiNZEu+NYRptC58DtSDcqu94izoGKIJZiKcAPPM/w4+EU0ie1bn2GyG2lwjiTiyM37PMRRorpa2zKLZpHDwNdQ6Y7odo2NlAmT1dvZOl05US9EIdkdEZzl/MNVnSzWjjxmV5s10yiDNwHjYl0pTR1bjd5DyalUUU6q81/JfWZbCiyuEp1AWZ3l1HUWqAjmgTO3Xd2+zw1MKgDu9gn1GT/UYHpyGHDYRQxYNzy9+31zc84XJlPlHVSwm4pt+aRjfu4NMwjq69p03n6S4R46cTLR8b9iqb/+AMbaXZ4KZW5kc3RyZWFtCmVuZG9iago0MSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDE2MyA+PgpzdHJlYW0KeJxNT02ygjEI2/cUOQJQfsp5dBwXev+t6Vfn+RadhAYSsFoQNN/UibKFu45VrN8bPASv0X8Y2kQKP1J+SMpCC3I6OpGhuA0VR7JB2Z09oa4oqa2kMU2hbVeqTbnwNizi/Ky9T5y9OE+l6eKTcbaULGBJfy+YrwvZY3p+xIjbnzW9mVic1gXlPtF1MPNS/HuPVfxj5+it/picYzc8x+MDVKw8SgplbmRzdHJlYW0KZW5kb2JqCjQyIDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMzU4ID4+CnN0cmVhbQp4nD2SO44eMQyD+zmFLjCAqZft8/xBkCK5f7ukvZtKhG2NyE9T1TYs3F4f23Jtqxj2C08Ne9Pt34Moe6vsL9W2d8GQrBjD0LwC3D4P1qT0ZY6miGleecXn8f0tw/u+ilw/nTE5abfF4t0MCz3O1M3mI3akRuGUXjpvmLsFgtb9BJCBZIVvntgeNjW4aFbT3LBPSu8zUs38ICxqWm3O3DxmS/IMIyxpRIaTxvjtCSsoGw4eV1doavBUKmisRtHjsiz1SBU2sR2o/xXTXkGC5M3szYSdlyj7DplIS0Z80eOCwTrjtBXQbIx5N+bczBF0ueNKZOhVMYn20yLRWk9ow4Qtr2e7n+fPAw9i2dq01CCAaIPQNSH4gUt6i/k9bep1qdDOOCJi6TYYBNhncWzkiKRLuVX1vlT0Ewg428g7WsxOT1z6IfrFWX0r/x5OlcLmymgahJU4tmg0Jm9cJmA/URTr9xdNFIc6CmVuZHN0cmVhbQplbmRvYmoKNDMgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAxNzcgPj4Kc3RyZWFtCnicTVBJjgQxCLvnFTyBNZD3dKvVh5r/X8dQpdEcEhtbAZOITUy7L6+ilE1vWVP/NLRwLQn/xzxOs+MUvEl9UxiT8Rl8LfO8lRIKVfJ2OOG4JfmBEkWegoOxHu0UVKkJYXlu1DNOIcy1nPkP23lYGtlBFw5wdNTuZqHT11B5BOmRwdfS/Sjek4tUkMgcTu/jyiSJFKwk+04CB2kN/9S7m/qg1rwB60zzX9f6rs8veoI/JQplbmRzdHJlYW0KZW5kb2JqCjQ0IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggNzEgPj4Kc3RyZWFtCnicMzIyUjBQMDMDEoamJgrmhmYKKYZcQL6ZoalCLogBEsrhgklCWCDJHJiqHK4MLrABYOWmhpZQRQiWAUSxAVhpGgDvJhYwCmVuZHN0cmVhbQplbmRvYmoKNDUgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCA1MCA+PgpzdHJlYW0KeJwzMjJSMFAwMwEShqZGCuaGZgophlxgfi6IAgnkcMGkICwDIA1WkcOVwZUGAJjYDJcKZW5kc3RyZWFtCmVuZG9iago0NiAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDI3NyA+PgpzdHJlYW0KeJxNUklywzAMu+sVfIK4U+9Jp5ND+v9rQSrp9GCDFkgQsF2qtCkCt4xDrpu+eM3zT4Pzoddi83+VWXQVTJZMXEJ2mGRjfCc9lijmREgiR1C3DD6W2r4nmOsOPejcAcZkQwWXFhnmzIB6mqkDBtp2VWLXWy3iqkWz7JSCuWoH6XDpZyJZM0D13tPVhvso/kPF3LtCpsgirQAiF5z3noBKUUDBep8huY0DwYkdeMIeN/K6+Fjuck+Q0rr/IA+3N0tDbkVK5EfH4OZmULUnRccHVfNTZcAf9m3HHFTF+432+zKwbLNPSt4OxPOeKBhLEsZObYZPT+PLZU5CDiTO9sZeo83eijEocYa5nubPeK3n+v4FPX1rEQplbmRzdHJlYW0KZW5kb2JqCjQ3IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMTg1ID4+CnN0cmVhbQp4nE1QMZIDMQjr/QqeYBB44T3JZK7I/b+NIJNMirUEMrLYiCNbTh+elwS23HVN/d8QWvJc6vHDnB9ZQmKrGHLGoHvwtuD67lzsmAuqfUDFzThjdLB5zoNup1o5yUrFL3atqPLG9lYyBJlzH1Fv1Jkh20yCqi9C48PohuIsHZE1nNnal1k6m1s7QpwbUEFvluPg4WJlg7dlPKdjOsm1WGvP6KEDK6UKr0HL3rRZZ5o/+Vx/6/ECJlhELQplbmRzdHJlYW0KZW5kb2JqCjQ4IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjM3ID4+CnN0cmVhbQp4nDVQQXLEMAy6+xX6QGcsCcnxe9LZ6aX/vxac9ASJhABXtU3LtC93K99WOe3bB/9EbfsVySojR7S5p2Gl3cNrcd8tPI4mVh+8R2IdhknZbEO5oTXB5hcvyCexD0YvTg6bk/vbnHpcRHJqetvigWQAwqTjMelU7vATXObCe8R8qjhgTOa6ecmgyKGizmtvu3v8DA+8TcV8cyEvyolM5i4z32VrWWRYP2Ytr2QSkuQTcppXvetwnAMIltg4GB2akGXoERhl4WgwlU9vDBZMPgSYWCY4yeUqphDaUKxHo6C56MrgGQ/+1/r8AcGHV6YKZW5kc3RyZWFtCmVuZG9iago0OSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDI4MCA+PgpzdHJlYW0KeJxNkUtuwzAMRPc+BS8QQPxKOk+KoIvk/ts+Kk3RhU16RM0Mx5klQ6rkpntJakn6kC+9PtCru9Qtz0vjfxep3VVIzCm6QPYU08HMkPtlHpLmYmWH0/ab+355jNP53MwuCXXuFicREza+pkmEgjK1Nyc5pnjO49DVTrXyPumuVUeJohULN9Y6UUuwFsgFLkeIWcsDQ4uBmyq23hXD9Ytg/JZwqkxgbb4N9RIONNkqGuZ9Anr+RfW8vk8yRqav0+niYvJgoRPSsVqIfSdjDBRyK7rgi7BonNu4dmA9QQbrahCKQbDjVKv20F3v0RMdpq88PVxJrCztTMQRWacinuONaCfjx2IcW1r9S0Dw5WbyWeXOWo8fD5Rm1gplbmRzdHJlYW0KZW5kb2JqCjUwIDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMTQ1ID4+CnN0cmVhbQp4nE2PuxEDMAhDe0/BCCDAn3mSy6Vw9m8j7BQpbMnWPXG4u6j05OUxJF3lae28PyVpS3aziD8XoeU63ehiE5KqAp40yKPBWIQQeD+FyKtM5nVuVPxGajH1E6heLPY6BMOpizSOkvbrYEn1MzFQtE0ypmJknLz1IT6ikqQLiCUTnUcx7CS1+b677vZury8m4TIBCmVuZHN0cmVhbQplbmRvYmoKNTEgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCA0MTIgPj4Kc3RyZWFtCnicLZNJciNBCEX3dQouoIhkyOk8cjh60b7/1u8jL1RQZCV/AM0xbFi6vdytltvMYV/+UPE57edxP+br2P/HI8wz+S3zwW8eO8fej59he1h4GA1idng/cTtJrq1rWWFnUk5qPqhvYvzFSp0oW2m5ANqHK9P8Dp0I9lIZaTEOGNMidOIXRBGqRXV0x+D++7kUdtneFvRYhUixmiBmGK2TJgpHZZIaZXSomKJdJbJbpkw7y+qIdlndEiyuTN7kxUS3r0G8bQZdAuxx20uRU8SP/cmS72fAB9G6K+FC5uRucGBCVbDQopOYFF0KzMLF/Ng4F9Ylc0kMzyuHRX604ZX9DXYVkgITimlFZUe4jOjMtyqaNf2zh8mzQsrohgbFvN4nZPv2DiQT9cLK1UMoRiPz521VvrE1d7vBt5ntRsoVcXU5qGdopOKFZ3mi54VmditYA2mPgjm6InYPiTtj9576iU+ccrAz6ebtzpa/NI32DAoXCmD06gk8rr2EH733YvXq7dD0lEkjnbRxVNknWEJDLn/+GO/n3/P9C7ekmQsKZW5kc3RyZWFtCmVuZG9iago1MiAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDE3MiA+PgpzdHJlYW0KeJxFUEkOAyEMu/MKf2AkYpaQ90xV9dD+/1qHqaYXbNkhNtAXKmzhmKAbvFY8rHAseOBTUjO8C/vA0UC2PVl7wlnMmcS649Bgq1ipGnOlaVczRENPdQ3MjkVE5GmDKRJ9VAVo/ibDQkTWTaYCZM3YBS92mdn0z34r5P6Z3XeN6uh6bh3Cjthl3RHSlaKGtlTOUo4JOayCASpBcBZyE3bC9Q/XN53lVZ5frhg9+wplbmRzdHJlYW0KZW5kb2JqCjUzIDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMTkzID4+CnN0cmVhbQp4nE1QS24FMQjb5xS+QKUAgcB5Wj11Mb3/9pnMPKkLZGQ+NrgHJmLhSwQrF1wKPzLWbP7v4A5cw8IhCZN5WnXJwe+hDyNJhj3uCNKiBdmQWTBFBXySTUMk9kIWgg3iJHsze2hCvA7Ubvo2cw1x/ZepyZNJtpwxepJali0cdvYKVbhHSsGzbp97cvwoqWcDaRaZGH2yamZ3t/EvnLatZ5kl0aoLxVNDYTxJGI39jK7EY/Pzxzubjeed1/gdrzd/jUT8CmVuZHN0cmVhbQplbmRvYmoKMTUgMCBvYmoKPDwgL0Jhc2VGb250IC9HWlJPTUsrQXJpYWxNVCAvQ2hhclByb2NzIDE2IDAgUgovRW5jb2RpbmcgPDwKL0RpZmZlcmVuY2VzIFsgMzIgL3VuaTAwMDAwMDAzIDQwIC91bmkwMDAwMDAwYiAvdW5pMDAwMDAwMGMgNDUgL3VuaTAwMDAwMDEwCi91bmkwMDAwMDAxMSA0OCAvdW5pMDAwMDAwMTMgL3VuaTAwMDAwMDE0IC91bmkwMDAwMDAxNSA1MiAvdW5pMDAwMDAwMTcKL3VuaTAwMDAwMDE4IC91bmkwMDAwMDAxOSAvdW5pMDAwMDAwMWEgL3VuaTAwMDAwMDFiIDY3IC91bmkwMDAwMDAyNiA3MwovdW5pMDAwMDAwMmMgNzYgL3VuaTAwMDAwMDJmIDgyIC91bmkwMDAwMDAzNSAvdW5pMDAwMDAwMzYgODcgL3VuaTAwMDAwMDNhCjk3IC91bmkwMDAwMDA0NCAvdW5pMDAwMDAwNDUgL3VuaTAwMDAwMDQ2IC91bmkwMDAwMDA0NyAvdW5pMDAwMDAwNDgKL3VuaTAwMDAwMDQ5IC91bmkwMDAwMDA0YSAvdW5pMDAwMDAwNGIgL3VuaTAwMDAwMDRjIDEwOCAvdW5pMDAwMDAwNGYKL3VuaTAwMDAwMDUwIC91bmkwMDAwMDA1MSAvdW5pMDAwMDAwNTIgL3VuaTAwMDAwMDUzIDExNCAvdW5pMDAwMDAwNTUKL3VuaTAwMDAwMDU2IC91bmkwMDAwMDA1NyAvdW5pMDAwMDAwNTggXQovVHlwZSAvRW5jb2RpbmcgPj4KL0ZpcnN0Q2hhciAwIC9Gb250QkJveCBbIC02NjUgLTMyNSAyMDAwIDEwNDAgXSAvRm9udERlc2NyaXB0b3IgMTQgMCBSCi9Gb250TWF0cml4IFsgMC4wMDEgMCAwIDAuMDAxIDAgMCBdIC9MYXN0Q2hhciAyNTUgL05hbWUgL0daUk9NSytBcmlhbE1UCi9TdWJ0eXBlIC9UeXBlMyAvVHlwZSAvRm9udCAvV2lkdGhzIDEzIDAgUiA+PgplbmRvYmoKMTQgMCBvYmoKPDwgL0FzY2VudCA5MDYgL0NhcEhlaWdodCA3MTYgL0Rlc2NlbnQgLTIxMiAvRmxhZ3MgMzIKL0ZvbnRCQm94IFsgLTY2NSAtMzI1IDIwMDAgMTA0MCBdIC9Gb250TmFtZSAvR1pST01LK0FyaWFsTVQgL0l0YWxpY0FuZ2xlIDAKL01heFdpZHRoIDEwMTUgL1N0ZW1WIDAgL1R5cGUgL0ZvbnREZXNjcmlwdG9yIC9YSGVpZ2h0IDUxOSA+PgplbmRvYmoKMTMgMCBvYmoKWyA3NTAgNzUwIDc1MCA3NTAgNzUwIDc1MCA3NTAgNzUwIDc1MCA3NTAgNzUwIDc1MCA3NTAgNzUwIDc1MCA3NTAgNzUwIDc1MAo3NTAgNzUwIDc1MCA3NTAgNzUwIDc1MCA3NTAgNzUwIDc1MCA3NTAgNzUwIDc1MCA3NTAgNzUwIDI3OCAyNzggMzU1IDU1NiA1NTYKODg5IDY2NyAxOTEgMzMzIDMzMyAzODkgNTg0IDI3OCAzMzMgMjc4IDI3OCA1NTYgNTU2IDU1NiA1NTYgNTU2IDU1NiA1NTYgNTU2CjU1NiA1NTYgMjc4IDI3OCA1ODQgNTg0IDU4NCA1NTYgMTAxNSA2NjcgNjY3IDcyMiA3MjIgNjY3IDYxMSA3NzggNzIyIDI3OAo1MDAgNjY3IDU1NiA4MzMgNzIyIDc3OCA2NjcgNzc4IDcyMiA2NjcgNjExIDcyMiA2NjcgOTQ0IDY2NyA2NjcgNjExIDI3OCAyNzgKMjc4IDQ2OSA1NTYgMzMzIDU1NiA1NTYgNTAwIDU1NiA1NTYgMjc4IDU1NiA1NTYgMjIyIDIyMiA1MDAgMjIyIDgzMyA1NTYgNTU2CjU1NiA1NTYgMzMzIDUwMCAyNzggNTU2IDUwMCA3MjIgNTAwIDUwMCA1MDAgMzM0IDI2MCAzMzQgNTg0IDc1MCA1NTYgNzUwIDIyMgo1NTYgMzMzIDEwMDAgNTU2IDU1NiAzMzMgMTAwMCA2NjcgMzMzIDEwMDAgNzUwIDYxMSA3NTAgNzUwIDIyMiAyMjIgMzMzIDMzMwozNTAgNTU2IDEwMDAgMzMzIDEwMDAgNTAwIDMzMyA5NDQgNzUwIDUwMCA2NjcgMjc4IDMzMyA1NTYgNTU2IDU1NiA1NTYgMjYwCjU1NiAzMzMgNzM3IDM3MCA1NTYgNTg0IDMzMyA3MzcgNTUyIDQwMCA1NDkgMzMzIDMzMyAzMzMgNTc2IDUzNyAzMzMgMzMzIDMzMwozNjUgNTU2IDgzNCA4MzQgODM0IDYxMSA2NjcgNjY3IDY2NyA2NjcgNjY3IDY2NyAxMDAwIDcyMiA2NjcgNjY3IDY2NyA2NjcKMjc4IDI3OCAyNzggMjc4IDcyMiA3MjIgNzc4IDc3OCA3NzggNzc4IDc3OCA1ODQgNzc4IDcyMiA3MjIgNzIyIDcyMiA2NjcgNjY3CjYxMSA1NTYgNTU2IDU1NiA1NTYgNTU2IDU1NiA4ODkgNTAwIDU1NiA1NTYgNTU2IDU1NiAyNzggMjc4IDI3OCAyNzggNTU2IDU1Ngo1NTYgNTU2IDU1NiA1NTYgNTU2IDU0OSA2MTEgNTU2IDU1NiA1NTYgNTU2IDUwMCA1NTYgNTAwIF0KZW5kb2JqCjE2IDAgb2JqCjw8IC91bmkwMDAwMDAwMyAxNyAwIFIgL3VuaTAwMDAwMDBiIDE4IDAgUiAvdW5pMDAwMDAwMGMgMTkgMCBSCi91bmkwMDAwMDAxMCAyMCAwIFIgL3VuaTAwMDAwMDExIDIxIDAgUiAvdW5pMDAwMDAwMTMgMjIgMCBSCi91bmkwMDAwMDAxNCAyMyAwIFIgL3VuaTAwMDAwMDE1IDI0IDAgUiAvdW5pMDAwMDAwMTcgMjUgMCBSCi91bmkwMDAwMDAxOCAyNiAwIFIgL3VuaTAwMDAwMDE5IDI3IDAgUiAvdW5pMDAwMDAwMWEgMjggMCBSCi91bmkwMDAwMDAxYiAyOSAwIFIgL3VuaTAwMDAwMDI2IDMwIDAgUiAvdW5pMDAwMDAwMmMgMzEgMCBSCi91bmkwMDAwMDAyZiAzMiAwIFIgL3VuaTAwMDAwMDM1IDMzIDAgUiAvdW5pMDAwMDAwMzYgMzQgMCBSCi91bmkwMDAwMDAzYSAzNSAwIFIgL3VuaTAwMDAwMDQ0IDM2IDAgUiAvdW5pMDAwMDAwNDUgMzcgMCBSCi91bmkwMDAwMDA0NiAzOCAwIFIgL3VuaTAwMDAwMDQ3IDM5IDAgUiAvdW5pMDAwMDAwNDggNDAgMCBSCi91bmkwMDAwMDA0OSA0MSAwIFIgL3VuaTAwMDAwMDRhIDQyIDAgUiAvdW5pMDAwMDAwNGIgNDMgMCBSCi91bmkwMDAwMDA0YyA0NCAwIFIgL3VuaTAwMDAwMDRmIDQ1IDAgUiAvdW5pMDAwMDAwNTAgNDYgMCBSCi91bmkwMDAwMDA1MSA0NyAwIFIgL3VuaTAwMDAwMDUyIDQ4IDAgUiAvdW5pMDAwMDAwNTMgNDkgMCBSCi91bmkwMDAwMDA1NSA1MCAwIFIgL3VuaTAwMDAwMDU2IDUxIDAgUiAvdW5pMDAwMDAwNTcgNTIgMCBSCi91bmkwMDAwMDA1OCA1MyAwIFIgPj4KZW5kb2JqCjMgMCBvYmoKPDwgL0YxIDE1IDAgUiA+PgplbmRvYmoKNCAwIG9iago8PCAvQTEgPDwgL0NBIDAgL1R5cGUgL0V4dEdTdGF0ZSAvY2EgMSA+PgovQTIgPDwgL0NBIDEgL1R5cGUgL0V4dEdTdGF0ZSAvY2EgMSA+PiA+PgplbmRvYmoKNSAwIG9iago8PCA+PgplbmRvYmoKNiAwIG9iago8PCA+PgplbmRvYmoKNyAwIG9iago8PCA+PgplbmRvYmoKMiAwIG9iago8PCAvQ291bnQgMSAvS2lkcyBbIDExIDAgUiBdIC9UeXBlIC9QYWdlcyA+PgplbmRvYmoKNTQgMCBvYmoKPDwgL0NyZWF0aW9uRGF0ZSAoRDoyMDIyMDQwODAwMTIwOS0wNycwMCcpCi9DcmVhdG9yIChNYXRwbG90bGliIHYzLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZykKL1Byb2R1Y2VyIChNYXRwbG90bGliIHBkZiBiYWNrZW5kIHYzLjUuMSkgPj4KZW5kb2JqCnhyZWYKMCA1NQowMDAwMDAwMDAwIDY1NTM1IGYgCjAwMDAwMDAwMTYgMDAwMDAgbiAKMDAwMDAxNTM5NiAwMDAwMCBuIAowMDAwMDE1MjAyIDAwMDAwIG4gCjAwMDAwMTUyMzQgMDAwMDAgbiAKMDAwMDAxNTMzMyAwMDAwMCBuIAowMDAwMDE1MzU0IDAwMDAwIG4gCjAwMDAwMTUzNzUgMDAwMDAgbiAKMDAwMDAwMDA2NSAwMDAwMCBuIAowMDAwMDAwMzQyIDAwMDAwIG4gCjAwMDAwMDE4MzAgMDAwMDAgbiAKMDAwMDAwMDIwOCAwMDAwMCBuIAowMDAwMDAxODA5IDAwMDAwIG4gCjAwMDAwMTMzODkgMDAwMDAgbiAKMDAwMDAxMzE4MiAwMDAwMCBuIAowMDAwMDEyMzYwIDAwMDAwIG4gCjAwMDAwMTQ0NDAgMDAwMDAgbiAKMDAwMDAwMTg1MCAwMDAwMCBuIAowMDAwMDAxOTQwIDAwMDAwIG4gCjAwMDAwMDIxNjUgMDAwMDAgbiAKMDAwMDAwMjM5MCAwMDAwMCBuIAowMDAwMDAyNTE0IDAwMDAwIG4gCjAwMDAwMDI2MzAgMDAwMDAgbiAKMDAwMDAwMjk3OCAwMDAwMCBuIAowMDAwMDAzMTY3IDAwMDAwIG4gCjAwMDAwMDM1MDkgMDAwMDAgbiAKMDAwMDAwMzY3NSAwMDAwMCBuIAowMDAwMDAzOTk3IDAwMDAwIG4gCjAwMDAwMDQ0MjggMDAwMDAgbiAKMDAwMDAwNDY0MiAwMDAwMCBuIAowMDAwMDA1MTM0IDAwMDAwIG4gCjAwMDAwMDU1MDIgMDAwMDAgbiAKMDAwMDAwNTYyNCAwMDAwMCBuIAowMDAwMDA1NzU1IDAwMDAwIG4gCjAwMDAwMDYxMDkgMDAwMDAgbiAKMDAwMDAwNjYyNCAwMDAwMCBuIAowMDAwMDA2ODY0IDAwMDAwIG4gCjAwMDAwMDczNzggMDAwMDAgbiAKMDAwMDAwNzcwNSAwMDAwMCBuIAowMDAwMDA4MDI2IDAwMDAwIG4gCjAwMDAwMDgzNjEgMDAwMDAgbiAKMDAwMDAwODY5MyAwMDAwMCBuIAowMDAwMDA4OTI5IDAwMDAwIG4gCjAwMDAwMDkzNjAgMDAwMDAgbiAKMDAwMDAwOTYxMCAwMDAwMCBuIAowMDAwMDA5NzUzIDAwMDAwIG4gCjAwMDAwMDk4NzUgMDAwMDAgbiAKMDAwMDAxMDIyNSAwMDAwMCBuIAowMDAwMDEwNDgzIDAwMDAwIG4gCjAwMDAwMTA3OTMgMDAwMDAgbiAKMDAwMDAxMTE0NiAwMDAwMCBuIAowMDAwMDExMzY0IDAwMDAwIG4gCjAwMDAwMTE4NDkgMDAwMDAgbiAKMDAwMDAxMjA5NCAwMDAwMCBuIAowMDAwMDE1NDU2IDAwMDAwIG4gCnRyYWlsZXIKPDwgL0luZm8gNTQgMCBSIC9Sb290IDEgMCBSIC9TaXplIDU1ID4+CnN0YXJ0eHJlZgoxNTYxMwolJUVPRgo=\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"500.85125pt\" height=\"227.036719pt\" viewBox=\"0 0 500.85125 227.036719\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-04-08T00:12:09.040257</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 227.036719 \n",
       "L 500.85125 227.036719 \n",
       "L 500.85125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 47.25125 185.015625 \n",
       "L 493.65125 185.015625 \n",
       "L 493.65125 21.935625 \n",
       "L 47.25125 21.935625 \n",
       "z\n",
       "\" style=\"fill: #eaeaf2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 67.542159 185.015625 \n",
       "L 67.542159 21.935625 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(64.483643 202.389219)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-30\" d=\"M 266 2259 \n",
       "Q 266 3072 433 3567 \n",
       "Q 600 4063 929 4331 \n",
       "Q 1259 4600 1759 4600 \n",
       "Q 2128 4600 2406 4451 \n",
       "Q 2684 4303 2865 4023 \n",
       "Q 3047 3744 3150 3342 \n",
       "Q 3253 2941 3253 2259 \n",
       "Q 3253 1453 3087 958 \n",
       "Q 2922 463 2592 192 \n",
       "Q 2263 -78 1759 -78 \n",
       "Q 1097 -78 719 397 \n",
       "Q 266 969 266 2259 \n",
       "z\n",
       "M 844 2259 \n",
       "Q 844 1131 1108 757 \n",
       "Q 1372 384 1759 384 \n",
       "Q 2147 384 2411 759 \n",
       "Q 2675 1134 2675 2259 \n",
       "Q 2675 3391 2411 3762 \n",
       "Q 2147 4134 1753 4134 \n",
       "Q 1366 4134 1134 3806 \n",
       "Q 844 3388 844 2259 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <path d=\"M 118.294808 185.015625 \n",
       "L 118.294808 21.935625 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 250 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(109.119261 202.389219)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-32\" d=\"M 3222 541 \n",
       "L 3222 0 \n",
       "L 194 0 \n",
       "Q 188 203 259 391 \n",
       "Q 375 700 629 1000 \n",
       "Q 884 1300 1366 1694 \n",
       "Q 2113 2306 2375 2664 \n",
       "Q 2638 3022 2638 3341 \n",
       "Q 2638 3675 2398 3904 \n",
       "Q 2159 4134 1775 4134 \n",
       "Q 1369 4134 1125 3890 \n",
       "Q 881 3647 878 3216 \n",
       "L 300 3275 \n",
       "Q 359 3922 746 4261 \n",
       "Q 1134 4600 1788 4600 \n",
       "Q 2447 4600 2831 4234 \n",
       "Q 3216 3869 3216 3328 \n",
       "Q 3216 3053 3103 2787 \n",
       "Q 2991 2522 2730 2228 \n",
       "Q 2469 1934 1863 1422 \n",
       "Q 1356 997 1212 845 \n",
       "Q 1069 694 975 541 \n",
       "L 3222 541 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-35\" d=\"M 266 1200 \n",
       "L 856 1250 \n",
       "Q 922 819 1161 601 \n",
       "Q 1400 384 1738 384 \n",
       "Q 2144 384 2425 690 \n",
       "Q 2706 997 2706 1503 \n",
       "Q 2706 1984 2436 2262 \n",
       "Q 2166 2541 1728 2541 \n",
       "Q 1456 2541 1237 2417 \n",
       "Q 1019 2294 894 2097 \n",
       "L 366 2166 \n",
       "L 809 4519 \n",
       "L 3088 4519 \n",
       "L 3088 3981 \n",
       "L 1259 3981 \n",
       "L 1013 2750 \n",
       "Q 1425 3038 1878 3038 \n",
       "Q 2478 3038 2890 2622 \n",
       "Q 3303 2206 3303 1553 \n",
       "Q 3303 931 2941 478 \n",
       "Q 2500 -78 1738 -78 \n",
       "Q 1113 -78 717 272 \n",
       "Q 322 622 266 1200 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-32\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 169.047457 185.015625 \n",
       "L 169.047457 21.935625 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 500 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(159.87191 202.389219)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-35\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <path d=\"M 219.800106 185.015625 \n",
       "L 219.800106 21.935625 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 750 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(210.624559 202.389219)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-37\" d=\"M 303 3981 \n",
       "L 303 4522 \n",
       "L 3269 4522 \n",
       "L 3269 4084 \n",
       "Q 2831 3619 2401 2847 \n",
       "Q 1972 2075 1738 1259 \n",
       "Q 1569 684 1522 0 \n",
       "L 944 0 \n",
       "Q 953 541 1156 1306 \n",
       "Q 1359 2072 1739 2783 \n",
       "Q 2119 3494 2547 3981 \n",
       "L 303 3981 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-37\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 270.552755 185.015625 \n",
       "L 270.552755 21.935625 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 1000 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(258.318693 202.389219)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-31\" d=\"M 2384 0 \n",
       "L 1822 0 \n",
       "L 1822 3584 \n",
       "Q 1619 3391 1289 3197 \n",
       "Q 959 3003 697 2906 \n",
       "L 697 3450 \n",
       "Q 1169 3672 1522 3987 \n",
       "Q 1875 4303 2022 4600 \n",
       "L 2384 4600 \n",
       "L 2384 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-31\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <path d=\"M 321.305404 185.015625 \n",
       "L 321.305404 21.935625 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1250 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(309.071342 202.389219)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-31\"/>\n",
       "       <use xlink:href=\"#ArialMT-32\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"111.230469\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 372.058053 185.015625 \n",
       "L 372.058053 21.935625 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 1500 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(359.823991 202.389219)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-31\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <path d=\"M 422.810702 185.015625 \n",
       "L 422.810702 21.935625 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1750 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(410.57664 202.389219)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-31\"/>\n",
       "       <use xlink:href=\"#ArialMT-37\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"111.230469\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 473.563352 185.015625 \n",
       "L 473.563352 21.935625 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 2000 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(461.329289 202.389219)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-32\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- Iterations (in batches) -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(212.76125 217.311094)scale(0.12 -0.12)\">\n",
       "      <defs>\n",
       "       <path id=\"ArialMT-49\" d=\"M 597 0 \n",
       "L 597 4581 \n",
       "L 1203 4581 \n",
       "L 1203 0 \n",
       "L 597 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-74\" d=\"M 1650 503 \n",
       "L 1731 6 \n",
       "Q 1494 -44 1306 -44 \n",
       "Q 1000 -44 831 53 \n",
       "Q 663 150 594 308 \n",
       "Q 525 466 525 972 \n",
       "L 525 2881 \n",
       "L 113 2881 \n",
       "L 113 3319 \n",
       "L 525 3319 \n",
       "L 525 4141 \n",
       "L 1084 4478 \n",
       "L 1084 3319 \n",
       "L 1650 3319 \n",
       "L 1650 2881 \n",
       "L 1084 2881 \n",
       "L 1084 941 \n",
       "Q 1084 700 1114 631 \n",
       "Q 1144 563 1211 522 \n",
       "Q 1278 481 1403 481 \n",
       "Q 1497 481 1650 503 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-65\" d=\"M 2694 1069 \n",
       "L 3275 997 \n",
       "Q 3138 488 2766 206 \n",
       "Q 2394 -75 1816 -75 \n",
       "Q 1088 -75 661 373 \n",
       "Q 234 822 234 1631 \n",
       "Q 234 2469 665 2931 \n",
       "Q 1097 3394 1784 3394 \n",
       "Q 2450 3394 2872 2941 \n",
       "Q 3294 2488 3294 1666 \n",
       "Q 3294 1616 3291 1516 \n",
       "L 816 1516 \n",
       "Q 847 969 1125 678 \n",
       "Q 1403 388 1819 388 \n",
       "Q 2128 388 2347 550 \n",
       "Q 2566 713 2694 1069 \n",
       "z\n",
       "M 847 1978 \n",
       "L 2700 1978 \n",
       "Q 2663 2397 2488 2606 \n",
       "Q 2219 2931 1791 2931 \n",
       "Q 1403 2931 1139 2672 \n",
       "Q 875 2413 847 1978 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-72\" d=\"M 416 0 \n",
       "L 416 3319 \n",
       "L 922 3319 \n",
       "L 922 2816 \n",
       "Q 1116 3169 1280 3281 \n",
       "Q 1444 3394 1641 3394 \n",
       "Q 1925 3394 2219 3213 \n",
       "L 2025 2691 \n",
       "Q 1819 2813 1613 2813 \n",
       "Q 1428 2813 1281 2702 \n",
       "Q 1134 2591 1072 2394 \n",
       "Q 978 2094 978 1738 \n",
       "L 978 0 \n",
       "L 416 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-61\" d=\"M 2588 409 \n",
       "Q 2275 144 1986 34 \n",
       "Q 1697 -75 1366 -75 \n",
       "Q 819 -75 525 192 \n",
       "Q 231 459 231 875 \n",
       "Q 231 1119 342 1320 \n",
       "Q 453 1522 633 1644 \n",
       "Q 813 1766 1038 1828 \n",
       "Q 1203 1872 1538 1913 \n",
       "Q 2219 1994 2541 2106 \n",
       "Q 2544 2222 2544 2253 \n",
       "Q 2544 2597 2384 2738 \n",
       "Q 2169 2928 1744 2928 \n",
       "Q 1347 2928 1158 2789 \n",
       "Q 969 2650 878 2297 \n",
       "L 328 2372 \n",
       "Q 403 2725 575 2942 \n",
       "Q 747 3159 1072 3276 \n",
       "Q 1397 3394 1825 3394 \n",
       "Q 2250 3394 2515 3294 \n",
       "Q 2781 3194 2906 3042 \n",
       "Q 3031 2891 3081 2659 \n",
       "Q 3109 2516 3109 2141 \n",
       "L 3109 1391 \n",
       "Q 3109 606 3145 398 \n",
       "Q 3181 191 3288 0 \n",
       "L 2700 0 \n",
       "Q 2613 175 2588 409 \n",
       "z\n",
       "M 2541 1666 \n",
       "Q 2234 1541 1622 1453 \n",
       "Q 1275 1403 1131 1340 \n",
       "Q 988 1278 909 1158 \n",
       "Q 831 1038 831 891 \n",
       "Q 831 666 1001 516 \n",
       "Q 1172 366 1500 366 \n",
       "Q 1825 366 2078 508 \n",
       "Q 2331 650 2450 897 \n",
       "Q 2541 1088 2541 1459 \n",
       "L 2541 1666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-69\" d=\"M 425 3934 \n",
       "L 425 4581 \n",
       "L 988 4581 \n",
       "L 988 3934 \n",
       "L 425 3934 \n",
       "z\n",
       "M 425 0 \n",
       "L 425 3319 \n",
       "L 988 3319 \n",
       "L 988 0 \n",
       "L 425 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-6f\" d=\"M 213 1659 \n",
       "Q 213 2581 725 3025 \n",
       "Q 1153 3394 1769 3394 \n",
       "Q 2453 3394 2887 2945 \n",
       "Q 3322 2497 3322 1706 \n",
       "Q 3322 1066 3130 698 \n",
       "Q 2938 331 2570 128 \n",
       "Q 2203 -75 1769 -75 \n",
       "Q 1072 -75 642 372 \n",
       "Q 213 819 213 1659 \n",
       "z\n",
       "M 791 1659 \n",
       "Q 791 1022 1069 705 \n",
       "Q 1347 388 1769 388 \n",
       "Q 2188 388 2466 706 \n",
       "Q 2744 1025 2744 1678 \n",
       "Q 2744 2294 2464 2611 \n",
       "Q 2184 2928 1769 2928 \n",
       "Q 1347 2928 1069 2612 \n",
       "Q 791 2297 791 1659 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-6e\" d=\"M 422 0 \n",
       "L 422 3319 \n",
       "L 928 3319 \n",
       "L 928 2847 \n",
       "Q 1294 3394 1984 3394 \n",
       "Q 2284 3394 2536 3286 \n",
       "Q 2788 3178 2913 3003 \n",
       "Q 3038 2828 3088 2588 \n",
       "Q 3119 2431 3119 2041 \n",
       "L 3119 0 \n",
       "L 2556 0 \n",
       "L 2556 2019 \n",
       "Q 2556 2363 2490 2533 \n",
       "Q 2425 2703 2258 2804 \n",
       "Q 2091 2906 1866 2906 \n",
       "Q 1506 2906 1245 2678 \n",
       "Q 984 2450 984 1813 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-73\" d=\"M 197 991 \n",
       "L 753 1078 \n",
       "Q 800 744 1014 566 \n",
       "Q 1228 388 1613 388 \n",
       "Q 2000 388 2187 545 \n",
       "Q 2375 703 2375 916 \n",
       "Q 2375 1106 2209 1216 \n",
       "Q 2094 1291 1634 1406 \n",
       "Q 1016 1563 777 1677 \n",
       "Q 538 1791 414 1992 \n",
       "Q 291 2194 291 2438 \n",
       "Q 291 2659 392 2848 \n",
       "Q 494 3038 669 3163 \n",
       "Q 800 3259 1026 3326 \n",
       "Q 1253 3394 1513 3394 \n",
       "Q 1903 3394 2198 3281 \n",
       "Q 2494 3169 2634 2976 \n",
       "Q 2775 2784 2828 2463 \n",
       "L 2278 2388 \n",
       "Q 2241 2644 2061 2787 \n",
       "Q 1881 2931 1553 2931 \n",
       "Q 1166 2931 1000 2803 \n",
       "Q 834 2675 834 2503 \n",
       "Q 834 2394 903 2306 \n",
       "Q 972 2216 1119 2156 \n",
       "Q 1203 2125 1616 2013 \n",
       "Q 2213 1853 2448 1751 \n",
       "Q 2684 1650 2818 1456 \n",
       "Q 2953 1263 2953 975 \n",
       "Q 2953 694 2789 445 \n",
       "Q 2625 197 2315 61 \n",
       "Q 2006 -75 1616 -75 \n",
       "Q 969 -75 630 194 \n",
       "Q 291 463 197 991 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-28\" d=\"M 1497 -1347 \n",
       "Q 1031 -759 709 28 \n",
       "Q 388 816 388 1659 \n",
       "Q 388 2403 628 3084 \n",
       "Q 909 3875 1497 4659 \n",
       "L 1900 4659 \n",
       "Q 1522 4009 1400 3731 \n",
       "Q 1209 3300 1100 2831 \n",
       "Q 966 2247 966 1656 \n",
       "Q 966 153 1900 -1347 \n",
       "L 1497 -1347 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-62\" d=\"M 941 0 \n",
       "L 419 0 \n",
       "L 419 4581 \n",
       "L 981 4581 \n",
       "L 981 2947 \n",
       "Q 1338 3394 1891 3394 \n",
       "Q 2197 3394 2470 3270 \n",
       "Q 2744 3147 2920 2923 \n",
       "Q 3097 2700 3197 2384 \n",
       "Q 3297 2069 3297 1709 \n",
       "Q 3297 856 2875 390 \n",
       "Q 2453 -75 1863 -75 \n",
       "Q 1275 -75 941 416 \n",
       "L 941 0 \n",
       "z\n",
       "M 934 1684 \n",
       "Q 934 1088 1097 822 \n",
       "Q 1363 388 1816 388 \n",
       "Q 2184 388 2453 708 \n",
       "Q 2722 1028 2722 1663 \n",
       "Q 2722 2313 2464 2622 \n",
       "Q 2206 2931 1841 2931 \n",
       "Q 1472 2931 1203 2611 \n",
       "Q 934 2291 934 1684 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-63\" d=\"M 2588 1216 \n",
       "L 3141 1144 \n",
       "Q 3050 572 2676 248 \n",
       "Q 2303 -75 1759 -75 \n",
       "Q 1078 -75 664 370 \n",
       "Q 250 816 250 1647 \n",
       "Q 250 2184 428 2587 \n",
       "Q 606 2991 970 3192 \n",
       "Q 1334 3394 1763 3394 \n",
       "Q 2303 3394 2647 3120 \n",
       "Q 2991 2847 3088 2344 \n",
       "L 2541 2259 \n",
       "Q 2463 2594 2264 2762 \n",
       "Q 2066 2931 1784 2931 \n",
       "Q 1359 2931 1093 2626 \n",
       "Q 828 2322 828 1663 \n",
       "Q 828 994 1084 691 \n",
       "Q 1341 388 1753 388 \n",
       "Q 2084 388 2306 591 \n",
       "Q 2528 794 2588 1216 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-68\" d=\"M 422 0 \n",
       "L 422 4581 \n",
       "L 984 4581 \n",
       "L 984 2938 \n",
       "Q 1378 3394 1978 3394 \n",
       "Q 2347 3394 2619 3248 \n",
       "Q 2891 3103 3008 2847 \n",
       "Q 3125 2591 3125 2103 \n",
       "L 3125 0 \n",
       "L 2563 0 \n",
       "L 2563 2103 \n",
       "Q 2563 2525 2380 2717 \n",
       "Q 2197 2909 1863 2909 \n",
       "Q 1613 2909 1392 2779 \n",
       "Q 1172 2650 1078 2428 \n",
       "Q 984 2206 984 1816 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-29\" d=\"M 791 -1347 \n",
       "L 388 -1347 \n",
       "Q 1322 153 1322 1656 \n",
       "Q 1322 2244 1188 2822 \n",
       "Q 1081 3291 891 3722 \n",
       "Q 769 4003 388 4659 \n",
       "L 791 4659 \n",
       "Q 1378 3875 1659 3084 \n",
       "Q 1900 2403 1900 1659 \n",
       "Q 1900 816 1576 28 \n",
       "Q 1253 -759 791 -1347 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#ArialMT-49\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"55.566406\"/>\n",
       "      <use xlink:href=\"#ArialMT-72\" x=\"111.181641\"/>\n",
       "      <use xlink:href=\"#ArialMT-61\" x=\"144.482422\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"200.097656\"/>\n",
       "      <use xlink:href=\"#ArialMT-69\" x=\"227.880859\"/>\n",
       "      <use xlink:href=\"#ArialMT-6f\" x=\"250.097656\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"305.712891\"/>\n",
       "      <use xlink:href=\"#ArialMT-73\" x=\"361.328125\"/>\n",
       "      <use xlink:href=\"#ArialMT-20\" x=\"411.328125\"/>\n",
       "      <use xlink:href=\"#ArialMT-28\" x=\"439.111328\"/>\n",
       "      <use xlink:href=\"#ArialMT-69\" x=\"472.412109\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"494.628906\"/>\n",
       "      <use xlink:href=\"#ArialMT-20\" x=\"550.244141\"/>\n",
       "      <use xlink:href=\"#ArialMT-62\" x=\"578.027344\"/>\n",
       "      <use xlink:href=\"#ArialMT-61\" x=\"633.642578\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"689.257812\"/>\n",
       "      <use xlink:href=\"#ArialMT-63\" x=\"717.041016\"/>\n",
       "      <use xlink:href=\"#ArialMT-68\" x=\"767.041016\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"822.65625\"/>\n",
       "      <use xlink:href=\"#ArialMT-73\" x=\"878.271484\"/>\n",
       "      <use xlink:href=\"#ArialMT-29\" x=\"928.271484\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <path d=\"M 47.25125 177.602898 \n",
       "L 493.65125 177.602898 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.46125 181.539695)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-2e\" d=\"M 581 0 \n",
       "L 581 641 \n",
       "L 1222 641 \n",
       "L 1222 0 \n",
       "L 581 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 47.25125 147.768332 \n",
       "L 493.65125 147.768332 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.2 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.46125 151.705129)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-32\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <path d=\"M 47.25125 117.933767 \n",
       "L 493.65125 117.933767 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.4 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.46125 121.870563)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-34\" d=\"M 2069 0 \n",
       "L 2069 1097 \n",
       "L 81 1097 \n",
       "L 81 1613 \n",
       "L 2172 4581 \n",
       "L 2631 4581 \n",
       "L 2631 1613 \n",
       "L 3250 1613 \n",
       "L 3250 1097 \n",
       "L 2631 1097 \n",
       "L 2631 0 \n",
       "L 2069 0 \n",
       "z\n",
       "M 2069 1613 \n",
       "L 2069 3678 \n",
       "L 634 1613 \n",
       "L 2069 1613 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-34\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 47.25125 88.099201 \n",
       "L 493.65125 88.099201 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.6 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.46125 92.035998)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-36\" d=\"M 3184 3459 \n",
       "L 2625 3416 \n",
       "Q 2550 3747 2413 3897 \n",
       "Q 2184 4138 1850 4138 \n",
       "Q 1581 4138 1378 3988 \n",
       "Q 1113 3794 959 3422 \n",
       "Q 806 3050 800 2363 \n",
       "Q 1003 2672 1297 2822 \n",
       "Q 1591 2972 1913 2972 \n",
       "Q 2475 2972 2870 2558 \n",
       "Q 3266 2144 3266 1488 \n",
       "Q 3266 1056 3080 686 \n",
       "Q 2894 316 2569 119 \n",
       "Q 2244 -78 1831 -78 \n",
       "Q 1128 -78 684 439 \n",
       "Q 241 956 241 2144 \n",
       "Q 241 3472 731 4075 \n",
       "Q 1159 4600 1884 4600 \n",
       "Q 2425 4600 2770 4297 \n",
       "Q 3116 3994 3184 3459 \n",
       "z\n",
       "M 888 1484 \n",
       "Q 888 1194 1011 928 \n",
       "Q 1134 663 1356 523 \n",
       "Q 1578 384 1822 384 \n",
       "Q 2178 384 2434 671 \n",
       "Q 2691 959 2691 1453 \n",
       "Q 2691 1928 2437 2201 \n",
       "Q 2184 2475 1800 2475 \n",
       "Q 1419 2475 1153 2201 \n",
       "Q 888 1928 888 1484 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-36\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <path d=\"M 47.25125 58.264635 \n",
       "L 493.65125 58.264635 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.8 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.46125 62.201432)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-38\" d=\"M 1131 2484 \n",
       "Q 781 2613 612 2850 \n",
       "Q 444 3088 444 3419 \n",
       "Q 444 3919 803 4259 \n",
       "Q 1163 4600 1759 4600 \n",
       "Q 2359 4600 2725 4251 \n",
       "Q 3091 3903 3091 3403 \n",
       "Q 3091 3084 2923 2848 \n",
       "Q 2756 2613 2416 2484 \n",
       "Q 2838 2347 3058 2040 \n",
       "Q 3278 1734 3278 1309 \n",
       "Q 3278 722 2862 322 \n",
       "Q 2447 -78 1769 -78 \n",
       "Q 1091 -78 675 323 \n",
       "Q 259 725 259 1325 \n",
       "Q 259 1772 486 2073 \n",
       "Q 713 2375 1131 2484 \n",
       "z\n",
       "M 1019 3438 \n",
       "Q 1019 3113 1228 2906 \n",
       "Q 1438 2700 1772 2700 \n",
       "Q 2097 2700 2305 2904 \n",
       "Q 2513 3109 2513 3406 \n",
       "Q 2513 3716 2298 3927 \n",
       "Q 2084 4138 1766 4138 \n",
       "Q 1444 4138 1231 3931 \n",
       "Q 1019 3725 1019 3438 \n",
       "z\n",
       "M 838 1322 \n",
       "Q 838 1081 952 856 \n",
       "Q 1066 631 1291 507 \n",
       "Q 1516 384 1775 384 \n",
       "Q 2178 384 2440 643 \n",
       "Q 2703 903 2703 1303 \n",
       "Q 2703 1709 2433 1975 \n",
       "Q 2163 2241 1756 2241 \n",
       "Q 1359 2241 1098 1978 \n",
       "Q 838 1716 838 1322 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-38\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 47.25125 28.43007 \n",
       "L 493.65125 28.43007 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 1.0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.46125 32.366867)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-31\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- Learning rate factor -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(15.935625 155.502187)rotate(-90)scale(0.12 -0.12)\">\n",
       "      <defs>\n",
       "       <path id=\"ArialMT-4c\" d=\"M 469 0 \n",
       "L 469 4581 \n",
       "L 1075 4581 \n",
       "L 1075 541 \n",
       "L 3331 541 \n",
       "L 3331 0 \n",
       "L 469 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-67\" d=\"M 319 -275 \n",
       "L 866 -356 \n",
       "Q 900 -609 1056 -725 \n",
       "Q 1266 -881 1628 -881 \n",
       "Q 2019 -881 2231 -725 \n",
       "Q 2444 -569 2519 -288 \n",
       "Q 2563 -116 2559 434 \n",
       "Q 2191 0 1641 0 \n",
       "Q 956 0 581 494 \n",
       "Q 206 988 206 1678 \n",
       "Q 206 2153 378 2554 \n",
       "Q 550 2956 876 3175 \n",
       "Q 1203 3394 1644 3394 \n",
       "Q 2231 3394 2613 2919 \n",
       "L 2613 3319 \n",
       "L 3131 3319 \n",
       "L 3131 450 \n",
       "Q 3131 -325 2973 -648 \n",
       "Q 2816 -972 2473 -1159 \n",
       "Q 2131 -1347 1631 -1347 \n",
       "Q 1038 -1347 672 -1080 \n",
       "Q 306 -813 319 -275 \n",
       "z\n",
       "M 784 1719 \n",
       "Q 784 1066 1043 766 \n",
       "Q 1303 466 1694 466 \n",
       "Q 2081 466 2343 764 \n",
       "Q 2606 1063 2606 1700 \n",
       "Q 2606 2309 2336 2618 \n",
       "Q 2066 2928 1684 2928 \n",
       "Q 1309 2928 1046 2623 \n",
       "Q 784 2319 784 1719 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-66\" d=\"M 556 0 \n",
       "L 556 2881 \n",
       "L 59 2881 \n",
       "L 59 3319 \n",
       "L 556 3319 \n",
       "L 556 3672 \n",
       "Q 556 4006 616 4169 \n",
       "Q 697 4388 901 4523 \n",
       "Q 1106 4659 1475 4659 \n",
       "Q 1713 4659 2000 4603 \n",
       "L 1916 4113 \n",
       "Q 1741 4144 1584 4144 \n",
       "Q 1328 4144 1222 4034 \n",
       "Q 1116 3925 1116 3625 \n",
       "L 1116 3319 \n",
       "L 1763 3319 \n",
       "L 1763 2881 \n",
       "L 1116 2881 \n",
       "L 1116 0 \n",
       "L 556 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#ArialMT-4c\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"55.615234\"/>\n",
       "      <use xlink:href=\"#ArialMT-61\" x=\"111.230469\"/>\n",
       "      <use xlink:href=\"#ArialMT-72\" x=\"166.845703\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"200.146484\"/>\n",
       "      <use xlink:href=\"#ArialMT-69\" x=\"255.761719\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"277.978516\"/>\n",
       "      <use xlink:href=\"#ArialMT-67\" x=\"333.59375\"/>\n",
       "      <use xlink:href=\"#ArialMT-20\" x=\"389.208984\"/>\n",
       "      <use xlink:href=\"#ArialMT-72\" x=\"416.992188\"/>\n",
       "      <use xlink:href=\"#ArialMT-61\" x=\"450.292969\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"505.908203\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"533.691406\"/>\n",
       "      <use xlink:href=\"#ArialMT-20\" x=\"589.306641\"/>\n",
       "      <use xlink:href=\"#ArialMT-66\" x=\"617.089844\"/>\n",
       "      <use xlink:href=\"#ArialMT-61\" x=\"644.873047\"/>\n",
       "      <use xlink:href=\"#ArialMT-63\" x=\"700.488281\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"750.488281\"/>\n",
       "      <use xlink:href=\"#ArialMT-6f\" x=\"778.271484\"/>\n",
       "      <use xlink:href=\"#ArialMT-72\" x=\"833.886719\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 67.542159 177.602898 \n",
       "L 87.031176 35.209552 \n",
       "L 87.843219 29.348352 \n",
       "L 94.94859 30.10081 \n",
       "L 102.05396 31.073605 \n",
       "L 109.362342 32.300963 \n",
       "L 116.670723 33.754391 \n",
       "L 124.182115 35.478878 \n",
       "L 131.693507 37.431433 \n",
       "L 139.40791 39.667229 \n",
       "L 147.325323 42.19641 \n",
       "L 155.445747 45.027384 \n",
       "L 163.769182 48.16654 \n",
       "L 172.295627 51.617956 \n",
       "L 181.228093 55.473314 \n",
       "L 190.56658 59.746335 \n",
       "L 200.5141 64.545863 \n",
       "L 211.070651 69.887725 \n",
       "L 222.642255 75.995663 \n",
       "L 235.634933 83.109221 \n",
       "L 251.469759 92.043412 \n",
       "L 278.87619 107.816726 \n",
       "L 300.598324 120.200405 \n",
       "L 315.215086 128.28173 \n",
       "L 327.598733 134.879805 \n",
       "L 338.764316 140.579932 \n",
       "L 349.117856 145.616025 \n",
       "L 358.862365 150.106825 \n",
       "L 367.997841 154.074398 \n",
       "L 376.727297 157.62764 \n",
       "L 385.253742 160.85793 \n",
       "L 393.374166 163.70062 \n",
       "L 401.291579 166.241741 \n",
       "L 409.005982 168.489618 \n",
       "L 416.720385 170.504289 \n",
       "L 424.231777 172.23502 \n",
       "L 431.540158 173.694665 \n",
       "L 438.84854 174.928359 \n",
       "L 446.156921 175.932157 \n",
       "L 453.262292 176.684615 \n",
       "L 460.367663 177.214462 \n",
       "L 467.473034 177.520097 \n",
       "L 473.360341 177.602806 \n",
       "L 473.360341 177.602806 \n",
       "\" clip-path=\"url(#p15b5172454)\" style=\"fill: none; stroke: #4c72b0; stroke-width: 1.5; stroke-linecap: round\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 47.25125 185.015625 \n",
       "L 47.25125 21.935625 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 493.65125 185.015625 \n",
       "L 493.65125 21.935625 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 47.25125 185.015625 \n",
       "L 493.65125 185.015625 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 47.25125 21.935625 \n",
       "L 493.65125 21.935625 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- Cosine Warm-up Learning Rate Scheduler -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(157.626875 15.935625)scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-43\" d=\"M 3763 1606 \n",
       "L 4369 1453 \n",
       "Q 4178 706 3683 314 \n",
       "Q 3188 -78 2472 -78 \n",
       "Q 1731 -78 1267 223 \n",
       "Q 803 525 561 1097 \n",
       "Q 319 1669 319 2325 \n",
       "Q 319 3041 592 3573 \n",
       "Q 866 4106 1370 4382 \n",
       "Q 1875 4659 2481 4659 \n",
       "Q 3169 4659 3637 4309 \n",
       "Q 4106 3959 4291 3325 \n",
       "L 3694 3184 \n",
       "Q 3534 3684 3231 3912 \n",
       "Q 2928 4141 2469 4141 \n",
       "Q 1941 4141 1586 3887 \n",
       "Q 1231 3634 1087 3207 \n",
       "Q 944 2781 944 2328 \n",
       "Q 944 1744 1114 1308 \n",
       "Q 1284 872 1643 656 \n",
       "Q 2003 441 2422 441 \n",
       "Q 2931 441 3284 734 \n",
       "Q 3638 1028 3763 1606 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-57\" d=\"M 1294 0 \n",
       "L 78 4581 \n",
       "L 700 4581 \n",
       "L 1397 1578 \n",
       "Q 1509 1106 1591 641 \n",
       "Q 1766 1375 1797 1488 \n",
       "L 2669 4581 \n",
       "L 3400 4581 \n",
       "L 4056 2263 \n",
       "Q 4303 1400 4413 641 \n",
       "Q 4500 1075 4641 1638 \n",
       "L 5359 4581 \n",
       "L 5969 4581 \n",
       "L 4713 0 \n",
       "L 4128 0 \n",
       "L 3163 3491 \n",
       "Q 3041 3928 3019 4028 \n",
       "Q 2947 3713 2884 3491 \n",
       "L 1913 0 \n",
       "L 1294 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-6d\" d=\"M 422 0 \n",
       "L 422 3319 \n",
       "L 925 3319 \n",
       "L 925 2853 \n",
       "Q 1081 3097 1340 3245 \n",
       "Q 1600 3394 1931 3394 \n",
       "Q 2300 3394 2536 3241 \n",
       "Q 2772 3088 2869 2813 \n",
       "Q 3263 3394 3894 3394 \n",
       "Q 4388 3394 4653 3120 \n",
       "Q 4919 2847 4919 2278 \n",
       "L 4919 0 \n",
       "L 4359 0 \n",
       "L 4359 2091 \n",
       "Q 4359 2428 4304 2576 \n",
       "Q 4250 2725 4106 2815 \n",
       "Q 3963 2906 3769 2906 \n",
       "Q 3419 2906 3187 2673 \n",
       "Q 2956 2441 2956 1928 \n",
       "L 2956 0 \n",
       "L 2394 0 \n",
       "L 2394 2156 \n",
       "Q 2394 2531 2256 2718 \n",
       "Q 2119 2906 1806 2906 \n",
       "Q 1569 2906 1367 2781 \n",
       "Q 1166 2656 1075 2415 \n",
       "Q 984 2175 984 1722 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-2d\" d=\"M 203 1375 \n",
       "L 203 1941 \n",
       "L 1931 1941 \n",
       "L 1931 1375 \n",
       "L 203 1375 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-75\" d=\"M 2597 0 \n",
       "L 2597 488 \n",
       "Q 2209 -75 1544 -75 \n",
       "Q 1250 -75 995 37 \n",
       "Q 741 150 617 320 \n",
       "Q 494 491 444 738 \n",
       "Q 409 903 409 1263 \n",
       "L 409 3319 \n",
       "L 972 3319 \n",
       "L 972 1478 \n",
       "Q 972 1038 1006 884 \n",
       "Q 1059 663 1231 536 \n",
       "Q 1403 409 1656 409 \n",
       "Q 1909 409 2131 539 \n",
       "Q 2353 669 2445 892 \n",
       "Q 2538 1116 2538 1541 \n",
       "L 2538 3319 \n",
       "L 3100 3319 \n",
       "L 3100 0 \n",
       "L 2597 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-70\" d=\"M 422 -1272 \n",
       "L 422 3319 \n",
       "L 934 3319 \n",
       "L 934 2888 \n",
       "Q 1116 3141 1344 3267 \n",
       "Q 1572 3394 1897 3394 \n",
       "Q 2322 3394 2647 3175 \n",
       "Q 2972 2956 3137 2557 \n",
       "Q 3303 2159 3303 1684 \n",
       "Q 3303 1175 3120 767 \n",
       "Q 2938 359 2589 142 \n",
       "Q 2241 -75 1856 -75 \n",
       "Q 1575 -75 1351 44 \n",
       "Q 1128 163 984 344 \n",
       "L 984 -1272 \n",
       "L 422 -1272 \n",
       "z\n",
       "M 931 1641 \n",
       "Q 931 1000 1190 694 \n",
       "Q 1450 388 1819 388 \n",
       "Q 2194 388 2461 705 \n",
       "Q 2728 1022 2728 1688 \n",
       "Q 2728 2322 2467 2637 \n",
       "Q 2206 2953 1844 2953 \n",
       "Q 1484 2953 1207 2617 \n",
       "Q 931 2281 931 1641 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-52\" d=\"M 503 0 \n",
       "L 503 4581 \n",
       "L 2534 4581 \n",
       "Q 3147 4581 3465 4457 \n",
       "Q 3784 4334 3975 4021 \n",
       "Q 4166 3709 4166 3331 \n",
       "Q 4166 2844 3850 2509 \n",
       "Q 3534 2175 2875 2084 \n",
       "Q 3116 1969 3241 1856 \n",
       "Q 3506 1613 3744 1247 \n",
       "L 4541 0 \n",
       "L 3778 0 \n",
       "L 3172 953 \n",
       "Q 2906 1366 2734 1584 \n",
       "Q 2563 1803 2427 1890 \n",
       "Q 2291 1978 2150 2013 \n",
       "Q 2047 2034 1813 2034 \n",
       "L 1109 2034 \n",
       "L 1109 0 \n",
       "L 503 0 \n",
       "z\n",
       "M 1109 2559 \n",
       "L 2413 2559 \n",
       "Q 2828 2559 3062 2645 \n",
       "Q 3297 2731 3419 2920 \n",
       "Q 3541 3109 3541 3331 \n",
       "Q 3541 3656 3305 3865 \n",
       "Q 3069 4075 2559 4075 \n",
       "L 1109 4075 \n",
       "L 1109 2559 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-53\" d=\"M 288 1472 \n",
       "L 859 1522 \n",
       "Q 900 1178 1048 958 \n",
       "Q 1197 738 1509 602 \n",
       "Q 1822 466 2213 466 \n",
       "Q 2559 466 2825 569 \n",
       "Q 3091 672 3220 851 \n",
       "Q 3350 1031 3350 1244 \n",
       "Q 3350 1459 3225 1620 \n",
       "Q 3100 1781 2813 1891 \n",
       "Q 2628 1963 1997 2114 \n",
       "Q 1366 2266 1113 2400 \n",
       "Q 784 2572 623 2826 \n",
       "Q 463 3081 463 3397 \n",
       "Q 463 3744 659 4045 \n",
       "Q 856 4347 1234 4503 \n",
       "Q 1613 4659 2075 4659 \n",
       "Q 2584 4659 2973 4495 \n",
       "Q 3363 4331 3572 4012 \n",
       "Q 3781 3694 3797 3291 \n",
       "L 3216 3247 \n",
       "Q 3169 3681 2898 3903 \n",
       "Q 2628 4125 2100 4125 \n",
       "Q 1550 4125 1298 3923 \n",
       "Q 1047 3722 1047 3438 \n",
       "Q 1047 3191 1225 3031 \n",
       "Q 1400 2872 2139 2705 \n",
       "Q 2878 2538 3153 2413 \n",
       "Q 3553 2228 3743 1945 \n",
       "Q 3934 1663 3934 1294 \n",
       "Q 3934 928 3725 604 \n",
       "Q 3516 281 3123 101 \n",
       "Q 2731 -78 2241 -78 \n",
       "Q 1619 -78 1198 103 \n",
       "Q 778 284 539 648 \n",
       "Q 300 1013 288 1472 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-64\" d=\"M 2575 0 \n",
       "L 2575 419 \n",
       "Q 2259 -75 1647 -75 \n",
       "Q 1250 -75 917 144 \n",
       "Q 584 363 401 755 \n",
       "Q 219 1147 219 1656 \n",
       "Q 219 2153 384 2558 \n",
       "Q 550 2963 881 3178 \n",
       "Q 1213 3394 1622 3394 \n",
       "Q 1922 3394 2156 3267 \n",
       "Q 2391 3141 2538 2938 \n",
       "L 2538 4581 \n",
       "L 3097 4581 \n",
       "L 3097 0 \n",
       "L 2575 0 \n",
       "z\n",
       "M 797 1656 \n",
       "Q 797 1019 1065 703 \n",
       "Q 1334 388 1700 388 \n",
       "Q 2069 388 2326 689 \n",
       "Q 2584 991 2584 1609 \n",
       "Q 2584 2291 2321 2609 \n",
       "Q 2059 2928 1675 2928 \n",
       "Q 1300 2928 1048 2622 \n",
       "Q 797 2316 797 1656 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-6c\" d=\"M 409 0 \n",
       "L 409 4581 \n",
       "L 972 4581 \n",
       "L 972 0 \n",
       "L 409 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-43\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"72.216797\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"127.832031\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"177.832031\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"200.048828\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"255.664062\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"311.279297\"/>\n",
       "     <use xlink:href=\"#ArialMT-57\" x=\"339.0625\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"429.697266\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"485.3125\"/>\n",
       "     <use xlink:href=\"#ArialMT-6d\" x=\"518.613281\"/>\n",
       "     <use xlink:href=\"#ArialMT-2d\" x=\"601.914062\"/>\n",
       "     <use xlink:href=\"#ArialMT-75\" x=\"635.214844\"/>\n",
       "     <use xlink:href=\"#ArialMT-70\" x=\"690.830078\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"746.445312\"/>\n",
       "     <use xlink:href=\"#ArialMT-4c\" x=\"774.228516\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"829.84375\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"885.458984\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"941.074219\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"974.375\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"1029.990234\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"1052.207031\"/>\n",
       "     <use xlink:href=\"#ArialMT-67\" x=\"1107.822266\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"1163.4375\"/>\n",
       "     <use xlink:href=\"#ArialMT-52\" x=\"1191.220703\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"1263.4375\"/>\n",
       "     <use xlink:href=\"#ArialMT-74\" x=\"1319.052734\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"1346.835938\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"1402.451172\"/>\n",
       "     <use xlink:href=\"#ArialMT-53\" x=\"1430.234375\"/>\n",
       "     <use xlink:href=\"#ArialMT-63\" x=\"1496.933594\"/>\n",
       "     <use xlink:href=\"#ArialMT-68\" x=\"1546.933594\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"1602.548828\"/>\n",
       "     <use xlink:href=\"#ArialMT-64\" x=\"1658.164062\"/>\n",
       "     <use xlink:href=\"#ArialMT-75\" x=\"1713.779297\"/>\n",
       "     <use xlink:href=\"#ArialMT-6c\" x=\"1769.394531\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"1791.611328\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"1847.226562\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p15b5172454\">\n",
       "   <rect x=\"47.25125\" y=\"21.935625\" width=\"446.4\" height=\"163.08\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Needed for initializing the lr scheduler\n",
    "p = nn.Parameter(torch.empty(4,4))\n",
    "optimizer = optim.Adam([p], lr=1e-3)\n",
    "lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=100, max_iters=2000)\n",
    "\n",
    "# Plotting\n",
    "epochs = list(range(2000))\n",
    "sns.set()\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(epochs, [lr_scheduler.get_lr_factor(e) for e in epochs])\n",
    "plt.ylabel(\"Learning rate factor\")\n",
    "plt.xlabel(\"Iterations (in batches)\")\n",
    "plt.title(\"Cosine Warm-up Learning Rate Scheduler\")\n",
    "plt.show()\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first 100 iterations, we increase the learning rate factor from 0 to 1, whereas for all later iterations, we decay it using the cosine wave. Pre-implementations of this scheduler can be found in the popular NLP Transformer library [huggingface](https://huggingface.co/transformers/main_classes/optimizer_schedules.html?highlight=cosine#transformers.get_cosine_schedule_with_warmup)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning Module\n",
    "\n",
    "Finally, we can embed the Transformer architecture into a PyTorch lightning module. From Tutorial 5, you know that PyTorch Lightning simplifies our training and test code, as well as structures the code nicely in separate functions. We will implement a template for a classifier based on the Transformer encoder. Thereby, we have a prediction output per sequence element. If we would need a classifier over the whole sequence, the common approach is to add an additional `[CLS]` token to the sequence, representing the classifier token. However, here we focus on tasks where we have an output per element. \n",
    "\n",
    "Additionally to the Transformer architecture, we add a small input network (maps input dimensions to model dimensions), the positional encoding, and an output network (transforms output encodings to predictions). We also add the learning rate scheduler, which takes a step each iteration instead of once per epoch. This is needed for the warmup and the smooth cosine decay. The training, validation, and test step is left empty for now and will be filled for our task-specific models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0, input_dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Hidden dimensionality of the input\n",
    "            model_dim - Hidden dimensionality to use inside the Transformer\n",
    "            num_classes - Number of classes to predict per sequence element\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention blocks\n",
    "            num_layers - Number of encoder blocks to use.\n",
    "            lr - Learning rate in the optimizer\n",
    "            warmup - Number of warmup steps. Usually between 50 and 500\n",
    "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
    "            dropout - Dropout to apply inside the model\n",
    "            input_dropout - Dropout to apply on the input features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self._create_model()\n",
    "\n",
    "    def _create_model(self):\n",
    "        # Input dim -> Model dim\n",
    "        self.input_net = nn.Sequential(\n",
    "            nn.Dropout(self.hparams.input_dropout),\n",
    "            nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n",
    "        )\n",
    "        # Positional encoding for sequences\n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n",
    "                                              input_dim=self.hparams.model_dim,\n",
    "                                              dim_feedforward=2*self.hparams.model_dim,\n",
    "                                              num_heads=self.hparams.num_heads,\n",
    "                                              dropout=self.hparams.dropout)\n",
    "        # Output classifier per sequence lement\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
    "            nn.LayerNorm(self.hparams.model_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.hparams.dropout),\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n",
    "        ) \n",
    "\n",
    "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features of shape [Batch, SeqLen, input_dim]\n",
    "            mask - Mask to apply on the attention outputs (optional)\n",
    "            add_positional_encoding - If True, we add the positional encoding to the input.\n",
    "                                      Might not be desired for some tasks.\n",
    "        \"\"\"\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        x = self.output_net(x)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
    "        \"\"\"\n",
    "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
    "        Input arguments same as the forward pass.\n",
    "        \"\"\"\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
    "        return attention_maps\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        \n",
    "        # Apply lr scheduler per step\n",
    "        lr_scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                             warmup=self.hparams.warmup, \n",
    "                                             max_iters=self.hparams.max_iters)\n",
    "        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "After having finished the implementation of the Transformer architecture, we can start experimenting and apply it to various tasks. In this notebook, we will focus on two tasks: parallel Sequence-to-Sequence, and set anomaly detection. The two tasks focus on different properties of the Transformer architecture, and we go through them below.\n",
    "\n",
    "### Sequence to Sequence\n",
    "\n",
    "A Sequence-to-Sequence task represents a task where the input _and_ the output is a sequence, not necessarily of the same length. Popular tasks in this domain include machine translation and summarization. For this, we usually have a Transformer encoder for interpreting the input sequence, and a decoder for generating the output in an autoregressive manner. Here, however, we will go back to a much simpler example task and use only the encoder. Given a sequence of $N$ numbers between $0$ and $M$, the task is to reverse the input sequence. In Numpy notation, if our input is $x$, the output should be $x$[::-1]. Although this task sounds very simple, RNNs can have issues with such because the task requires long-term dependencies. Transformers are built to support such, and hence, we expect it to perform very well. \n",
    "\n",
    "First, let's create a dataset class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "        \n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = torch.flip(inp_data, dims=(0,))\n",
    "        return inp_data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an arbitrary number of random sequences of numbers between 0 and `num_categories-1`. The label is simply the tensor flipped over the sequence dimension. We can create the corresponding data loaders below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = partial(ReverseDataset, 10, 16)\n",
    "train_loader = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader   = data.DataLoader(dataset(1000), batch_size=128)\n",
    "test_loader  = data.DataLoader(dataset(10000), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an arbitrary sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([9, 6, 2, 0, 6, 2, 7, 9, 7, 3, 3, 4, 3, 7, 0, 9])\n",
      "Labels:     tensor([9, 0, 7, 3, 4, 3, 3, 7, 9, 7, 2, 6, 0, 2, 6, 9])\n"
     ]
    }
   ],
   "source": [
    "inp_data, labels = train_loader.dataset[0]\n",
    "print(\"Input data:\", inp_data)\n",
    "print(\"Labels:    \", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we pass the input sequence through the Transformer encoder and predict the output for each input token. We use the standard Cross-Entropy loss to perform this. Every number is represented as a one-hot vector. Remember that representing the categories as single scalars decreases the expressiveness of the model extremely as $0$ and $1$ are not closer related than $0$ and $9$ in our example. An alternative to a one-hot vector is using a learned embedding vector as it is provided by the PyTorch module `nn.Embedding`. However, using a one-hot vector with an additional linear layer as in our case has the same effect as an embedding layer (`self.input_net` maps one-hot vector to a dense vector, where each row of the weight matrix represents the embedding for a specific category).\n",
    "\n",
    "To implement the training dynamic, we create a new class inheriting from `TransformerPredictor` and overwriting the training, validation and test step functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversePredictor(TransformerPredictor):\n",
    "    \n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        # Fetch data and transform categories to one-hot vectors\n",
    "        inp_data, labels = batch\n",
    "        inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n",
    "        \n",
    "        # Perform prediction and calculate loss and accuracy\n",
    "        preds = self.forward(inp_data, add_positional_encoding=True)\n",
    "        loss = F.cross_entropy(preds.view(-1,preds.size(-1)), labels.view(-1))\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        \n",
    "        # Logging\n",
    "        self.log(f\"{mode}_loss\", loss)\n",
    "        self.log(f\"{mode}_acc\", acc)\n",
    "        return loss, acc\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a training function similar to the one we have seen in Tutorial 5 for PyTorch Lightning. We create a `pl.Trainer` object, running for $N$ epochs, logging in TensorBoard, and saving our best model based on the validation. Afterward, we test our models on the test set. An additional parameter we pass to the trainer here is `gradient_clip_val`. This clips the norm of the gradients for all parameters before taking an optimizer step and prevents the model from diverging if we obtain very high gradients at, for instance, sharp loss surfaces (see many good blog posts on gradient clipping, like [DeepAI glossary](https://deepai.org/machine-learning-glossary-and-terms/gradient-clipping)). For Transformers, gradient clipping can help to further stabilize the training during the first few iterations, and also afterward. In plain PyTorch, you can apply gradient clipping via `torch.nn.utils.clip_grad_norm_(...)` (see [documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_)). The clip value is usually between 0.5 and 10, depending on how harsh you want to clip large gradients. After having explained this, let's implement the training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reverse(**kwargs):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"ReverseTask\")\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir, \n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         gpus=1 if str(device).startswith(\"cuda\") else 0, \n",
    "                         max_epochs=10,\n",
    "                         gradient_clip_val=5,\n",
    "                         progress_bar_refresh_rate=1)\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "    \n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ReverseTask.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = ReversePredictor.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = ReversePredictor(max_iters=trainer.max_epochs*len(train_loader), **kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        \n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train the model. In this setup, we will use a single encoder block and a single head in the Multi-Head Attention. This is chosen because of the simplicity of the task, and in this case, the attention can actually be interpreted as an \"explanation\" of the predictions (compared to the other papers above dealing with deep Transformers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch_lit\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model, loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch_lit\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc64434dcdc341b3ab569b1e7e3c9144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287d77704a354c12bd15e63ce3c80ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reverse_model, reverse_result = train_reverse(input_dim=train_loader.dataset.num_categories,\n",
    "                                              model_dim=32,\n",
    "                                              num_heads=1,\n",
    "                                              num_classes=train_loader.dataset.num_categories,\n",
    "                                              num_layers=1,\n",
    "                                              dropout=0.0,\n",
    "                                              lr=5e-4,\n",
    "                                              warmup=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning of PyTorch Lightning regarding the number of workers can be ignored for now. As the data set is so simple and the `__getitem__` finishes a neglectable time, we don't need subprocesses to provide us the data (in fact, more workers can slow down the training as we have communication overhead among processes/threads). First, let's print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy:  100.00%\n",
      "Test accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Val accuracy:  {(100.0 * reverse_result['val_acc']):4.2f}%\")\n",
    "print(f\"Test accuracy: {(100.0 * reverse_result['test_acc']):4.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would have expected, the Transformer can correctly solve the task. However, how does the attention in the Multi-Head Attention block looks like for an arbitrary input? Let's try to visualize it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=reverse_model.hparams.num_classes).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = reverse_model.get_attention_maps(inp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `attention_maps` is a list of length $N$ where $N$ is the number of layers. Each element is a tensor of shape [Batch, Heads, SeqLen, SeqLen], which we can verify below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 16, 16])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_maps[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will write a plotting function that takes as input the sequences, attention maps, and an index indicating for which batch element we want to visualize the attention map. We will create a plot where over rows, we have different layers, while over columns, we show the different heads. Remember that the softmax has been applied for each row separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_maps(input_data, attn_maps, idx=0):\n",
    "    if input_data is not None:\n",
    "        input_data = input_data[idx].detach().cpu().numpy()\n",
    "    else:\n",
    "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
    "    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n",
    "    \n",
    "    num_heads = attn_maps[0].shape[0]\n",
    "    num_layers = len(attn_maps)\n",
    "    seq_len = input_data.shape[0]\n",
    "    fig_size = 4 if num_heads == 1 else 3\n",
    "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n",
    "    if num_layers == 1:\n",
    "        ax = [ax]\n",
    "    if num_heads == 1:\n",
    "        ax = [[a] for a in ax]\n",
    "    for row in range(num_layers):\n",
    "        for column in range(num_heads):\n",
    "            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n",
    "            ax[row][column].set_xticks(list(range(seq_len)))\n",
    "            ax[row][column].set_xticklabels(input_data.tolist())\n",
    "            ax[row][column].set_yticks(list(range(seq_len)))\n",
    "            ax[row][column].set_yticklabels(input_data.tolist())\n",
    "            ax[row][column].set_title(f\"Layer {row+1}, Head {column+1}\")\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the attention map of our trained Transformer on the reverse task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"245.2025pt\" height=\"263.63625pt\" viewBox=\"0 0 245.2025 263.63625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-04-08T00:12:31.616040</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 263.63625 \n",
       "L 245.2025 263.63625 \n",
       "L 245.2025 0 \n",
       "L 0 0 \n",
       "L 0 263.63625 \n",
       "z\n",
       "\" style=\"fill: none\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 20.5625 239.758125 \n",
       "L 238.0025 239.758125 \n",
       "L 238.0025 22.318125 \n",
       "L 20.5625 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pbee65debb1)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAEqElEQVR4nO3cv6vXdRTH8feVjyJXk2voRRolAk2KAklnKbOShoIKikAkMy4qhILXoHRRyJSiu9w/wL1BcSlcawmUQhraSqU06IcVerWtwUF4H/R1/erjsR/Od3ne93LuZ6yt3nqzwShbtKw0tnPTZPfM8fe/Ke1aUJoCuggNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUHAMN8/AP43LC6NPb9mojQ39drl7plX9j1V2uVFgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQ4KiYu6NwILxlXf8nultr7eCOf0tz+z/v/5T4ybOXSru8aBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAa73ub1hvDS29cmV3TOHp34v7dp19KHS3FfnL/YPzdX+U8CLBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgwFHxg6Twme4Xn+g/Dm6ttYM7rnbP7P20ehxc+0x39UC4wosGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBDgen8ULVpWGtuydqJ75pM9v5V2vXukf9eZ8xdKu9qNa7W5IC8aBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA6/07ZazwN2sYL6169enlpbltL/f/xumZ2q4zPxS+hz8CV/hVXjQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgGOim9VOQ5urbWFS7tHNj02UVr1wfY/S3PTM0u6Z06d+7W0q13/pzZ3n/KiQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoEuN6/VeEKv7XWXlrX/+nsvW/NlXZ9NFv7jafOFT7T7Qr/jvCiQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoE3L/X+8N4aez19Q+X5t58of9v1rETpVXti29/qQ26xJ83XjQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgGjcVQ8LO4eeW7titKq6W1/lOb2fbake+b091dKu9r1q7U55o0XDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgIHu9v2BhaWzz45PdMx/v+qu0a//M0tLc6e8u9Q/5RPcDw4sGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CCgflRcOBDe+OgjpVWzBy52z7x3ZFVp18mzhePg1hwIc1teNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDgKEN46XBZ9es6J45PPV3adfuo/2X+K7wuZd40SBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoY3NkyWBj9850r3zNuHlpd2ff3jz/1DN66VdsHd4EWDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAwNnfhmZuVwZ2HNnbPzH75U2WVz3Qz8rxoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYB/wE2T3TmDAslHAAAAABJRU5ErkJggg==\" id=\"imaged33600c074\" transform=\"scale(1 -1)translate(0 -218)\" x=\"20.5625\" y=\"-21.758125\" width=\"218\" height=\"218\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m60a739a08c\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"27.3575\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 9 -->\n",
       "      <g transform=\"translate(24.17625 254.356563)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"40.9475\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(37.76625 254.356563)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"54.5375\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 7 -->\n",
       "      <g transform=\"translate(51.35625 254.356563)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"68.1275\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(64.94625 254.356563)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"81.7175\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(78.53625 254.356563)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"95.3075\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(92.12625 254.356563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"108.8975\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(105.71625 254.356563)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"122.4875\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(119.30625 254.356563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"136.0775\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(132.89625 254.356563)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"149.6675\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(146.48625 254.356563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_11\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"163.2575\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(160.07625 254.356563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_12\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"176.8475\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(173.66625 254.356563)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_13\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"190.4375\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(187.25625 254.356563)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_14\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"204.0275\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(200.84625 254.356563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_15\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"217.6175\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(214.43625 254.356563)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_16\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m60a739a08c\" x=\"231.2075\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(228.02625 254.356563)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <defs>\n",
       "       <path id=\"m5347599ca3\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"232.963125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 9 -->\n",
       "      <g transform=\"translate(7.2 236.762344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"219.373125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(7.2 223.172344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"205.783125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 7 -->\n",
       "      <g transform=\"translate(7.2 209.582344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"192.193125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(7.2 195.992344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"178.603125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_21\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(7.2 182.402344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"165.013125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(7.2 168.812344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"151.423125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(7.2 155.222344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"137.833125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(7.2 141.632344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"124.243125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(7.2 128.042344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"110.653125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_26\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(7.2 114.452344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"97.063125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_27\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(7.2 100.862344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"83.473125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_28\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(7.2 87.272344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"69.883125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_29\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(7.2 73.682344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_14\">\n",
       "     <g id=\"line2d_30\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"56.293125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_30\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(7.2 60.092344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_15\">\n",
       "     <g id=\"line2d_31\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"42.703125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_31\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(7.2 46.502344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_16\">\n",
       "     <g id=\"line2d_32\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5347599ca3\" x=\"20.5625\" y=\"29.113125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_32\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(7.2 32.912344)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 20.5625 239.758125 \n",
       "L 20.5625 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 238.0025 239.758125 \n",
       "L 238.0025 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 20.5625 239.758125 \n",
       "L 238.0025 239.758125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 20.5625 22.318125 \n",
       "L 238.0025 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_33\">\n",
       "    <!-- Layer 1, Head 1 -->\n",
       "    <g transform=\"translate(81.600313 16.318125)scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \n",
       "L 1259 4666 \n",
       "L 1259 531 \n",
       "L 3531 531 \n",
       "L 3531 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-2c\" d=\"M 750 794 \n",
       "L 1409 794 \n",
       "L 1409 256 \n",
       "L 897 -744 \n",
       "L 494 -744 \n",
       "L 750 256 \n",
       "L 750 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-48\" d=\"M 628 4666 \n",
       "L 1259 4666 \n",
       "L 1259 2753 \n",
       "L 3553 2753 \n",
       "L 3553 4666 \n",
       "L 4184 4666 \n",
       "L 4184 0 \n",
       "L 3553 0 \n",
       "L 3553 2222 \n",
       "L 1259 2222 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-4c\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"55.712891\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-79\" x=\"116.992188\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"176.171875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"237.695312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"278.808594\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" x=\"310.595703\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-2c\" x=\"374.21875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"406.005859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-48\" x=\"437.792969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"512.988281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"574.511719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"635.791016\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"699.267578\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" x=\"731.054688\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pbee65debb1\">\n",
       "   <rect x=\"20.5625\" y=\"22.318125\" width=\"217.44\" height=\"217.44\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has learned to attend to the token that is on the flipped index of itself. Hence, it actually does what we intended it to do. We see that it however also pays some attention to values close to the flipped index. This is because the model doesn't need the perfect, hard attention to solve this problem, but is fine with this approximate, noisy attention map. The close-by indices are caused by the similarity of the positional encoding, which we also intended with the positional encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Anomaly Detection\n",
    "\n",
    "Besides sequences, sets are another data structure that is relevant for many applications. In contrast to sequences, elements are unordered in a set. RNNs can only be applied on sets by assuming an order in the data, which however biases the model towards a non-existing order in the data. [Vinyals et al. (2015)](https://arxiv.org/abs/1511.06391) and other papers have shown that the assumed order can have a significant impact on the model's performance, and hence, we should try to not use RNNs on sets. Ideally, our model should be permutation-equivariant/invariant such that the output is the same no matter how we sort the elements in a set. \n",
    "\n",
    "Transformers offer the perfect architecture for this as the Multi-Head Attention is permutation-equivariant, and thus, outputs the same values no matter in what order we enter the inputs (inputs and outputs are permuted equally). The task we are looking at for sets is _Set Anomaly Detection_ which means that we try to find the element(s) in a set that does not fit the others. In the research community, the common application of anomaly detection is performed on a set of images, where $N-1$ images belong to the same category/have the same high-level features while one belongs to another category. Note that category does not necessarily have to relate to a class in a standard classification problem, but could be the combination of multiple features. For instance, on a face dataset, this could be people with glasses, male, beard, etc. An example of distinguishing different animals can be seen below. The first four images show foxes, while the last represents a different animal. We want to recognize that the last image shows a different animal, but it is not relevant which class of animal it is.\n",
    "\n",
    "<center width=\"100%\" style=\"padding:20px\"><img src=\"cifar100_example_anomaly.png\" width=\"600px\"></center>\n",
    "\n",
    "In this tutorial, we will use the CIFAR100 dataset. CIFAR100 has 600 images for 100 classes each with a resolution of 32x32, similar to CIFAR10. The larger amount of classes requires the model to attend to specific features in the images instead of coarse features as in CIFAR10, therefore making the task harder. We will show the model a set of 9 images of one class, and 1 image from another class. The task is to find the image that is from a different class than the other images.\n",
    "Using the raw images directly as input to the Transformer is not a good idea, because it is not translation invariant as a CNN, and would need to learn to detect image features from high-dimensional input first of all. Instead, we will use a pre-trained ResNet34 model from the torchvision package to obtain high-level, low-dimensional features of the images. The ResNet model has been pre-trained on the [ImageNet](http://image-net.org/) dataset which contains 1 million images of 1k classes and varying resolutions. However, during training and testing, the images are usually scaled to a resolution of 224x224, and hence we rescale our CIFAR images to this resolution as well. Below, we will load the dataset, and prepare the data for being processed by the ResNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# ImageNet statistics\n",
    "DATA_MEANS = np.array([0.485, 0.456, 0.406])\n",
    "DATA_STD = np.array([0.229, 0.224, 0.225])\n",
    "# As torch tensors for later preprocessing\n",
    "TORCH_DATA_MEANS = torch.from_numpy(DATA_MEANS).view(1,3,1,1)\n",
    "TORCH_DATA_STD = torch.from_numpy(DATA_STD).view(1,3,1,1)\n",
    "\n",
    "# Resize to 224x224, and normalize to ImageNet statistic\n",
    "transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                ])\n",
    "# Loading the training dataset. \n",
    "train_set = CIFAR100(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR100(root=DATASET_PATH, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to run the pre-trained ResNet model on the images, and extract the features before the classification layer. These are the most high-level features, and should sufficiently describe the images. CIFAR100 has some similarity to ImageNet, and thus we are not retraining the ResNet model in any form. However, if you would want to get the best performance and have a very large dataset, it would be better to add the ResNet to the computation graph during training and finetune its parameters as well. As we don't have a large enough dataset and want to train our model efficiently, we will extract the features beforehand. Let's load and prepare the model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\n",
    "pretrained_model = torchvision.models.resnet34(pretrained=True)\n",
    "# Remove classification layer\n",
    "# In some models, it is called \"fc\", others have \"classifier\"\n",
    "# Setting both to an empty sequential represents an identity map of the final features.\n",
    "pretrained_model.fc = nn.Sequential()\n",
    "pretrained_model.classifier = nn.Sequential()\n",
    "# To GPU\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "# Only eval, no gradient required\n",
    "pretrained_model.eval()\n",
    "for p in pretrained_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write a extraction function for the features below. This cell requires access to a GPU, as the model is rather deep and the images relatively large. The GPUs on GoogleColab are sufficient, but running this cell can take 2-3 minutes. Once it is run, the features are exported on disk so they don't have to be recalculated every time you run the notebook. However, this requires >150MB free disk space. So it is recommended to run this only on a local computer if you have enough free disk and a GPU (GoogleColab is fine for this). If you do not have a GPU, you can download the features from the [GoogleDrive folder](https://drive.google.com/drive/folders/1DF7POc6j03pRiWQPWSl5QJX5iY-xK0sV?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_features(dataset, save_file):\n",
    "    if not os.path.isfile(save_file):\n",
    "        data_loader = data.DataLoader(dataset, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "        extracted_features = []\n",
    "        for imgs, _ in tqdm(data_loader):\n",
    "            imgs = imgs.to(device)\n",
    "            feats = pretrained_model(imgs)\n",
    "            extracted_features.append(feats)\n",
    "        extracted_features = torch.cat(extracted_features, dim=0)\n",
    "        extracted_features = extracted_features.detach().cpu()\n",
    "        torch.save(extracted_features, save_file)\n",
    "    else:\n",
    "        extracted_features = torch.load(save_file)\n",
    "    return extracted_features\n",
    "\n",
    "train_feat_file = os.path.join(CHECKPOINT_PATH, \"train_set_features.tar\")\n",
    "train_set_feats = extract_features(train_set, train_feat_file)\n",
    "\n",
    "test_feat_file = os.path.join(CHECKPOINT_PATH, \"test_set_features.tar\")\n",
    "test_feats = extract_features(test_set, test_feat_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the feature shapes below. The training should have 50k elements, and the test 10k images. The feature dimension is 512 for the ResNet34. If you experiment with other models, you likely see a different feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: torch.Size([50000, 512])\n",
      "Test:  torch.Size([10000, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", train_set_feats.shape)\n",
    "print(\"Test: \", test_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we want to create a validation set to detect when we should stop training. In this case, we will split the training set into 90% training, 10% validation. However, the difficulty is here that we need to ensure that the validation set has the same number of images for all 100 labels. Otherwise, we have a class imbalance which is not good for creating the image sets. Hence, we take 10% of the images for each class, and move them into the validation set. The code below does exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split train into train+val\n",
    "# Get labels from train set\n",
    "labels = train_set.targets\n",
    "\n",
    "# Get indices of images per class\n",
    "labels = torch.LongTensor(labels)\n",
    "num_labels = labels.max()+1\n",
    "sorted_indices = torch.argsort(labels).reshape(num_labels, -1) # [classes, num_imgs per class]\n",
    "\n",
    "# Determine number of validation images per class\n",
    "num_val_exmps = sorted_indices.shape[1] // 10\n",
    "\n",
    "# Get image indices for validation and training\n",
    "val_indices   = sorted_indices[:,:num_val_exmps].reshape(-1)\n",
    "train_indices = sorted_indices[:,num_val_exmps:].reshape(-1)\n",
    "\n",
    "# Group corresponding image features and labels\n",
    "train_feats, train_labels = train_set_feats[train_indices], labels[train_indices]\n",
    "val_feats,   val_labels   = train_set_feats[val_indices],   labels[val_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can prepare a dataset class for the set anomaly task. We define an epoch to be the sequence in which each image has been exactly once as an \"anomaly\". Hence, the length of the dataset is the number of images in it. For the training set, each time we access an item with `__getitem__`, we sample a random, different class than the image at the corresponding index `idx` has. In a second step, we sample $N-1$ images of this sampled class. The set of 10 images is finally returned. The randomness in the `__getitem__` allows us to see a slightly different set during each iteration. However, we can't use the same strategy for the test set as we want the test dataset to be the same every time we iterate over it. Hence, we sample the sets in the `__init__` method, and return those in `__getitem__`. The code below implements exactly this dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAnomalyDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, img_feats, labels, set_size=10, train=True):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            img_feats - Tensor of shape [num_imgs, img_dim]. Represents the high-level features.\n",
    "            labels - Tensor of shape [num_imgs], containing the class labels for the images\n",
    "            set_size - Number of elements in a set. N-1 are sampled from one class, and one from another one.\n",
    "            train - If True, a new set will be sampled every time __getitem__ is called.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_feats = img_feats\n",
    "        self.labels = labels\n",
    "        self.set_size = set_size-1 # The set size is here the size of correct images\n",
    "        self.train = train\n",
    "        \n",
    "        # Tensors with indices of the images per class\n",
    "        self.num_labels = labels.max()+1\n",
    "        self.img_idx_by_label = torch.argsort(self.labels).reshape(self.num_labels, -1)\n",
    "        \n",
    "        if not train:\n",
    "            self.test_sets = self._create_test_sets()\n",
    "            \n",
    "            \n",
    "    def _create_test_sets(self):\n",
    "        # Pre-generates the sets for each image for the test set\n",
    "        test_sets = []\n",
    "        num_imgs = self.img_feats.shape[0]\n",
    "        np.random.seed(42)\n",
    "        test_sets = [self.sample_img_set(self.labels[idx]) for idx in range(num_imgs)]\n",
    "        test_sets = torch.stack(test_sets, dim=0)\n",
    "        return test_sets\n",
    "            \n",
    "        \n",
    "    def sample_img_set(self, anomaly_label):\n",
    "        \"\"\"\n",
    "        Samples a new set of images, given the label of the anomaly. \n",
    "        The sampled images come from a different class than anomaly_label\n",
    "        \"\"\"\n",
    "        # Sample class from 0,...,num_classes-1 while skipping anomaly_label as class\n",
    "        set_label = np.random.randint(self.num_labels-1)\n",
    "        if set_label >= anomaly_label:\n",
    "            set_label += 1\n",
    "            \n",
    "        # Sample images from the class determined above\n",
    "        img_indices = np.random.choice(self.img_idx_by_label.shape[1], size=self.set_size, replace=False)\n",
    "        img_indices = self.img_idx_by_label[set_label, img_indices]\n",
    "        return img_indices\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.img_feats.shape[0]\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anomaly = self.img_feats[idx]\n",
    "        if self.train: # If train => sample\n",
    "            img_indices = self.sample_img_set(self.labels[idx])\n",
    "        else: # If test => use pre-generated ones\n",
    "            img_indices = self.test_sets[idx]\n",
    "            \n",
    "        # Concatenate images. The anomaly is always the last image for simplicity\n",
    "        img_set = torch.cat([self.img_feats[img_indices], anomaly[None]], dim=0)\n",
    "        indices = torch.cat([img_indices, torch.LongTensor([idx])], dim=0)\n",
    "        label = img_set.shape[0]-1\n",
    "        \n",
    "        # We return the indices of the images for visualization purpose. \"Label\" is the index of the anomaly\n",
    "        return img_set, indices, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can setup our datasets and data loaders below. Here, we will use a set size of 10, i.e. 9 images from one category + 1 anomaly. Feel free to change it if you want to experiment with the sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "SET_SIZE = 10\n",
    "test_labels = torch.LongTensor(test_set.targets)\n",
    "\n",
    "train_anom_dataset = SetAnomalyDataset(train_feats, train_labels, set_size=SET_SIZE, train=True)\n",
    "val_anom_dataset   = SetAnomalyDataset(val_feats,   val_labels,   set_size=SET_SIZE, train=False)\n",
    "test_anom_dataset  = SetAnomalyDataset(test_feats,  test_labels,  set_size=SET_SIZE, train=False)\n",
    "\n",
    "train_anom_loader = data.DataLoader(train_anom_dataset, batch_size=64, shuffle=True,  drop_last=True,  num_workers=4, pin_memory=True)\n",
    "val_anom_loader   = data.DataLoader(val_anom_dataset,   batch_size=64, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_anom_loader  = data.DataLoader(test_anom_dataset,  batch_size=64, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the dataset a little better, we can plot below a few sets from the test dataset. Each row shows a different input set, where the first 9 are from the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 22260, 24400, 29056, 34448) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch_lit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1011\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1011\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch_lit\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     14\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m---> 16\u001b[0m _, indices, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_anom_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m visualize_exmp(indices[:\u001b[38;5;241m4\u001b[39m], test_set)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch_lit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch_lit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1207\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1207\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch_lit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1173\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1173\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1174\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1175\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch_lit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1024\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1023\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 22260, 24400, 29056, 34448) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "def visualize_exmp(indices, orig_dataset):\n",
    "    images = [orig_dataset[idx][0] for idx in indices.reshape(-1)]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    images = images * TORCH_DATA_STD + TORCH_DATA_MEANS\n",
    "    \n",
    "    img_grid = torchvision.utils.make_grid(images, nrow=SET_SIZE, normalize=True, pad_value=0.5, padding=16)\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title(\"Anomaly examples on CIFAR100\")\n",
    "    plt.imshow(img_grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "_, indices, _ = next(iter(test_anom_loader))\n",
    "visualize_exmp(indices[:4], test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see that for some sets the task might be easier than for others. Difficulties can especially arise if the anomaly is in a different, but yet visually similar class (e.g. train vs bus, flour vs worm, etc.).\n",
    "\n",
    "After having prepared the data, we can look closer at the model. Here, we have a classification of the whole set. For the prediction to be permutation-equivariant, we will output one logit for each image. Over these logits, we apply a softmax and train the anomaly image to have the highest score/probability. This is a bit different than a standard classification layer as the softmax is applied over images, not over output classes in the classical sense. However, if we swap two images in their position, we effectively swap their position in the output softmax. Hence, the prediction is equivariant with respect to the input. We implement this idea below in the subclass of the Transformer Lightning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyPredictor(TransformerPredictor):\n",
    "    \n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        img_sets, _, labels = batch\n",
    "        preds = self.forward(img_sets, add_positional_encoding=False) # No positional encodings as it is a set, not a sequence!\n",
    "        preds = preds.squeeze(dim=-1) # Shape: [Batch_size, set_size]\n",
    "        loss = F.cross_entropy(preds, labels) # Softmax/CE over set dimension\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log(f\"{mode}_loss\", loss)\n",
    "        self.log(f\"{mode}_acc\", acc, on_step=False, on_epoch=True)\n",
    "        return loss, acc\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write our train function below. It has the exact same structure as the reverse task one, hence not much of an explanation is needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_anomaly(**kwargs):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"SetAnomalyTask\")\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir, \n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         gpus=1 if str(device).startswith(\"cuda\") else 0, \n",
    "                         max_epochs=100,\n",
    "                         gradient_clip_val=2,\n",
    "                         progress_bar_refresh_rate=1)\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "    \n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"SetAnomalyTask.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = AnomalyPredictor.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = AnomalyPredictor(max_iters=trainer.max_epochs*len(train_anom_loader), **kwargs)\n",
    "        trainer.fit(model, train_anom_loader, val_anom_loader)\n",
    "        model = AnomalyPredictor.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    \n",
    "    # Test best model on validation and test set\n",
    "    train_result = trainer.test(model, dataloaders=train_anom_loader, verbose=False)\n",
    "    val_result = trainer.test(model, dataloaders=val_anom_loader, verbose=False)\n",
    "    test_result = trainer.test(model, dataloaders=test_anom_loader, verbose=False)\n",
    "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"], \"train_acc\": train_result[0][\"test_acc\"]}\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally train our model. We will use 4 layers with 4 attention heads each. The hidden dimensionality of the model is 256, and we use a dropout of 0.1 throughout the model for good regularization. Note that we also apply the dropout on the input features, as this makes the model more robust against image noise and generalizes better. Again, we use warmup to slowly start our model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_model, anomaly_result = train_anomaly(input_dim=train_anom_dataset.img_feats.shape[-1],\n",
    "                                              model_dim=256,\n",
    "                                              num_heads=4,\n",
    "                                              num_classes=1,\n",
    "                                              num_layers=4,\n",
    "                                              dropout=0.1,\n",
    "                                              input_dropout=0.1,\n",
    "                                              lr=5e-4,\n",
    "                                              warmup=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the achieved accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train accuracy: {(100.0*anomaly_result['train_acc']):4.2f}%\")\n",
    "print(f\"Val accuracy:   {(100.0*anomaly_result['val_acc']):4.2f}%\")\n",
    "print(f\"Test accuracy:  {(100.0*anomaly_result['test_acc']):4.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ~94% validation and test accuracy, the model generalizes quite well. It should be noted that you might see slightly different scores depending on what computer/device you are running this notebook. This is because despite setting the seed before generating the test dataset, it is not the same across platforms and numpy versions. Nevertheless, we can conclude that the model performs quite well and can solve the task for most sets. Before trying to interpret the model, let's verify that our model is permutation-equivariant, and assigns the same predictions for different permutations of the input set. For this, we sample a batch from the test set and run it through the model to obtain the probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data, indices, labels = next(iter(test_anom_loader))\n",
    "inp_data = inp_data.to(device)\n",
    "\n",
    "anomaly_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = anomaly_model.forward(inp_data, add_positional_encoding=False)\n",
    "    preds = F.softmax(preds.squeeze(dim=-1), dim=-1)\n",
    "\n",
    "    # Permut input data\n",
    "    permut = np.random.permutation(inp_data.shape[1])\n",
    "    perm_inp_data = inp_data[:,permut]\n",
    "    perm_preds = anomaly_model.forward(perm_inp_data, add_positional_encoding=False)\n",
    "    perm_preds = F.softmax(perm_preds.squeeze(dim=-1), dim=-1)\n",
    "\n",
    "assert (preds[:,permut] - perm_preds).abs().max() < 1e-5, \"Predictions are not permutation equivariant\"\n",
    "\n",
    "print(\"Preds\\n\", preds[0,permut].cpu().numpy())\n",
    "print(\"Permuted preds\\n\", perm_preds[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the predictions are almost exactly the same, and only differ because of slight numerical differences inside the network operation.\n",
    "\n",
    "To interpret the model a little more, we can plot the attention maps inside the model. This will give us an idea of what information the model is sharing/communicating between images, and what each head might represent. First, we need to extract the attention maps for the test batch above, and determine the discrete predictions for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_maps = anomaly_model.get_attention_maps(inp_data, add_positional_encoding=False)\n",
    "predictions = preds.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we write a plot function which plots the images in the input set, the prediction of the model, and the attention maps of the different heads on layers of the transformer. Feel free to explore the attention maps for different input examples as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(idx):\n",
    "    visualize_exmp(indices[idx:idx+1], test_set)\n",
    "    print(\"Prediction:\", predictions[idx].item())\n",
    "    plot_attention_maps(input_data=None, attn_maps=attention_maps, idx=idx)\n",
    "\n",
    "visualize_prediction(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the random seed, you might see a slightly different input set. For the version on the website, we compare 9 tree images with a volcano. We see that multiple heads, for instance, Layer 2 Head 1, Layer 2 Head 3, and Layer 3 Head 1 focus on the last image. Additionally, the heads in Layer 4 all seem to ignore the last image and assign a very low attention probability to it. This shows that the model has indeed recognized that the image doesn't fit the setting, and hence predicted it to be the anomaly. Layer 3 Head 2-4 seems to take a slightly weighted average of all images. That might indicate that the model extracts the \"average\" information of all images, to compare it to the image features itself. \n",
    "\n",
    "Let's try to find where the model actually makes a mistake. We can do this by identifying the sets where the model predicts something else than 9, as in the dataset, we ensured that the anomaly is always at the last position in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = torch.where(predictions != 9)[0].cpu().numpy()\n",
    "print(\"Indices with mistake:\", mistakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our model achieves ~94% accuracy, we only have very little number of mistakes in a batch of 64 sets. Still, let's visualize one of them, for example the last one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(mistakes[-1])\n",
    "print(\"Probabilities:\")\n",
    "for i, p in enumerate(preds[mistakes[-1]].cpu().numpy()):\n",
    "    print(f\"Image {i}: {100.0*p:4.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the model confuses a palm tree with a building, giving a probability of ~90% to image 2, and 8% to the actual anomaly. However, the difficulty here is that the picture of the building has been taken at a similar angle as the palms. Meanwhile, image 2 shows a rather unusual palm with a different color palette, which is why the model fails here. Nevertheless, in general, the model performs quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this tutorial, we took a closer look at the Multi-Head Attention layer which uses a scaled dot product between queries and keys to find correlations and similarities between input elements. The Transformer architecture is based on the Multi-Head Attention layer and applies multiple of them in a ResNet-like block. The Transformer is a very important, recent architecture that can be applied to many tasks and datasets. Although it is best known for its success in NLP, there is so much more to it. We have seen its application on sequence-to-sequence tasks and set anomaly detection. Its property of being permutation-equivariant if we do not provide any positional encodings, allows it to generalize to many settings. Hence, it is important to know the architecture, but also its possible issues such as the gradient problem during the first iterations solved by learning rate warm-up. If you are interested in continuing with the study of the Transformer architecture, please have a look at the blog posts listed at the beginning of the tutorial notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
